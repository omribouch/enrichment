{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537a8c4591fc1b2a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:46:59.074241Z",
     "start_time": "2025-09-07T14:46:59.070720Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from math import isnan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from ds_aws_services.athena_api  import cachedAthenaApi\n",
    "from ds_aws_services import CachedAthenaApi\n",
    "import pandas as pd\n",
    "# from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "os.environ['disk_caching'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2ebad395a59b2",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e0932fbb2c306",
   "metadata": {},
   "source": [
    "### Params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3bd542ed2d3eb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:40:45.098660Z",
     "start_time": "2025-09-07T15:40:45.096239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Biz2credit BI\n",
    "partner_ids = [13589]\n",
    "process_names = [\"bi_biz2credit_lead\"]\n",
    "transaction_month_prt = \"2025-01\"\n",
    "vertical_ids = [\"64e33e7be3cbc4ce1041a30f\"]\n",
    "start_date = \"2025-01-01\"\n",
    "end_date = \"2025-09-06\"\n",
    "transaction_month_prt_start = \"2025-01\"\n",
    "transaction_month_prt_end = \"2025-10\"\n",
    "\n",
    "enrichment_cols = \"age_of_business_months, application_annual_revenue, business_legal_structure, loan_purpose, industry, sub_industry, users_prob_sale, \"\n",
    "max_enrichment_cols = \"MAX(age_of_business_months) AS age_of_business_months, MAX(application_annual_revenue) AS application_annual_revenue, max(business_legal_structure) as business_legal_structure, max(loan_purpose) as loan_purpose, max(industry) as industry, max(sub_industry) as sub_industry, max(users_prob_sale) as users_prob_sale \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfb5a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rocket \n",
    "partner_ids = [3158,3178]\n",
    "process_names = [\"quicken_rocket_allocated\"]\n",
    "vertical_ids = [\"5fa2b415c91a2010c3432900\"]\n",
    "start_date = \"2025-01-01\"\n",
    "end_date = \"2025-09-09\"\n",
    "transaction_month_prt = \"2025-01\"\n",
    "# transaction_month_prt_start = \"2025-08\"\n",
    "# transaction_month_prt_end = \"2025-10\"\n",
    "\n",
    "enrich_cond = \"AND loanpurpose = 'Refinance'\"\n",
    "enrichment_cols = \"loanamount, creditscore, \"\n",
    "max_enrichment_cols = \"MAX(f.loanamount)  AS loan_amount, MAX(f.creditscore) AS credit_score \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8ada34f7b78",
   "metadata": {},
   "source": [
    "### Query 1 :  Checking RN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b82b6fa10dd4e0da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:47:05.720283Z",
     "start_time": "2025-09-07T14:47:05.713295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-09 11:16:32,551 INFO [ds_logger.py:68] [Cached AthenaApi] Local disk-caching is ENABLED.\n",
      "2025-09-09 11:16:32,551 INFO [ds_logger.py:68] [Cached AthenaApi] Using cached results for execute_fetch(args=(\"\\nWITH enrichment_data AS (\\n    SELECT  f.subid,\\n            process_name,\\n            f.partner_name,\\n            f.rn,\\n            transaction_month_prt,\\n            company,\\n            MIN(f.rn) OVER (PARTITION BY subid) AS min_rn,\\n            MAX(f.loanamount)  AS loan_amount, MAX(f.creditscore) AS credit_score \\n\\n    FROM dlk_visitor_funnel_dwh_production.enrich_conversions_flatten f\\n    WHERE f.partner_id in (3158,3178)\\n    AND process_name in ('quicken_rocket_allocated')\\n     AND transaction_month_prt >= '2025-01'\\n     AND f.vertical_id in ('5fa2b415c91a2010c3432900')\\n\\n    GROUP BY 1,2,3,4,5,6\\n)\\nSELECT\\n       rn,\\n       COUNT(DISTINCT subid) AS cids,\\n       COUNT(subid)          AS rowss\\nFROM enrichment_data\\nWHERE rn = min_rn\\nGROUP BY 1\\nORDER BY 2 DESC\\n\",), kwargs={}).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rn</th>\n",
       "      <th>cids</th>\n",
       "      <th>rowss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>82127</td>\n",
       "      <td>82136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rn   cids  rowss\n",
       "0   1  82127  82136"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_data1(\n",
    "    partner_ids,\n",
    "    process_names,              # e.g. ['quicken_rocket_allocated','quicken_rocket_leads']\n",
    "    transaction_month_prt,      # e.g. '2025-02' (or None to derive from start_date)\n",
    "    vertical_ids,\n",
    "    start_date,                 # e.g. '2025-08-01'\n",
    "    end_date,                   # not used in this query, kept for params symmetry\n",
    "    cond1_col=None,             # e.g. 'loanpurpose'\n",
    "    cond1_val=None              # e.g. 'Refinance' or '%%' to skip\n",
    ") -> pd.DataFrame:\n",
    "    # --- Build WHERE for the CTE ---\n",
    "    query = f\"\"\"\n",
    "WITH enrichment_data AS (\n",
    "    SELECT  f.subid,\n",
    "            process_name,\n",
    "            f.partner_name,\n",
    "            f.rn,\n",
    "            transaction_month_prt,\n",
    "            company,\n",
    "            MIN(f.rn) OVER (PARTITION BY subid) AS min_rn,\n",
    "            {max_enrichment_cols}\n",
    "\n",
    "    FROM dlk_visitor_funnel_dwh_production.enrich_conversions_flatten f\n",
    "    WHERE f.partner_id in ({','.join(map(str, partner_ids))})\n",
    "    AND process_name in ({','.join(f\"'{p}'\" for p in process_names)})\n",
    "     AND transaction_month_prt >= '{transaction_month_prt}'\n",
    "     AND f.vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    "\n",
    "    GROUP BY 1,2,3,4,5,6\n",
    ")\n",
    "SELECT\n",
    "       rn,\n",
    "       COUNT(DISTINCT subid) AS cids,\n",
    "       COUNT(subid)          AS rowss\n",
    "FROM enrichment_data\n",
    "WHERE rn = min_rn\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "\"\"\"\n",
    "    return pd.DataFrame(CachedAthenaApi().execute_fetch(query))\n",
    "\n",
    "df1 = get_data1(\n",
    "    partner_ids=partner_ids,\n",
    "    process_names=process_names,\n",
    "    transaction_month_prt=transaction_month_prt,\n",
    "    vertical_ids=vertical_ids,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date\n",
    ")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475543e0fffcd1fa",
   "metadata": {},
   "source": [
    "### Query 2: checking dates Last Updates (Enrichment Action Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c371dc11c66fec53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:27:42.219223Z",
     "start_time": "2025-09-07T15:27:42.216156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-09 11:16:33,156 INFO [ds_logger.py:68] [Cached AthenaApi] Local disk-caching is ENABLED.\n",
      "2025-09-09 11:16:33,157 INFO [ds_logger.py:68] [Cached AthenaApi] Using cached results for execute_fetch(args=(\"\\nSELECT\\n  CAST(SUBSTRING(action_time, 1, 10) AS DATE) AS action_day,\\n  COUNT(DISTINCT subid) AS subids\\nFROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\\nWHERE partner_id in (3158,3178)\\nAND process_name in ('quicken_rocket_allocated')\\n AND transaction_month_prt >= '2025-01'\\n AND vertical_id in ('5fa2b415c91a2010c3432900')\\nGROUP BY 1\\nORDER BY 1 DESC\\n\",), kwargs={}).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_day</th>\n",
       "      <th>subids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-07</td>\n",
       "      <td>1671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>9532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-01</td>\n",
       "      <td>7732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_day  subids\n",
       "0  2025-09-07    1671\n",
       "1  2025-09-01    9532\n",
       "2  2025-08-26       1\n",
       "3  2025-08-14       1\n",
       "4  2025-08-01    7732"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data2(\n",
    "    partner_ids,\n",
    "    process_names,\n",
    "    transaction_month_prt,\n",
    "    vertical_ids,\n",
    "    start_date,\n",
    "    end_date\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    query = f\"\"\"\n",
    "SELECT\n",
    "  CAST(SUBSTRING(action_time, 1, 10) AS DATE) AS action_day,\n",
    "  COUNT(DISTINCT subid) AS subids\n",
    "FROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\n",
    "WHERE partner_id in ({','.join(map(str, partner_ids))})\n",
    "AND process_name in ({','.join(f\"'{p}'\" for p in process_names)})\n",
    " AND transaction_month_prt >= '{transaction_month_prt}'\n",
    " AND vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    "GROUP BY 1\n",
    "ORDER BY 1 DESC\n",
    "\"\"\"\n",
    "    return pd.DataFrame(CachedAthenaApi().execute_fetch(query))\n",
    "\n",
    "df2 = get_data2(\n",
    "    partner_ids=partner_ids,\n",
    "    process_names=process_names,\n",
    "    transaction_month_prt=transaction_month_prt,\n",
    "    vertical_ids=vertical_ids,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date\n",
    ")\n",
    "df2.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed228ff8",
   "metadata": {},
   "source": [
    "Most subids are updated in the last day! The ones that has previous rows data is because the values changed so they have bigger RN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5af35c",
   "metadata": {},
   "source": [
    "### Query 3: Per creation date counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7d528b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-09 11:16:39,198 INFO [ds_logger.py:68] [Cached AthenaApi] Local disk-caching is ENABLED.\n",
      "2025-09-09 11:16:39,199 INFO [ds_logger.py:68] [Cached AthenaApi] Using cached results for execute_fetch(args=(\"\\nSELECT\\n  CAST(SUBSTRING(created_at, 1, 10) AS DATE) AS created_at_day,\\n  COUNT(DISTINCT subid) AS subids\\nFROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\\nWHERE partner_id in (3158,3178)\\nAND process_name in ('quicken_rocket_allocated')\\n AND transaction_month_prt >= '2025-01'\\n AND vertical_id in ('5fa2b415c91a2010c3432900')\\n -- and rn = 1\\nGROUP BY 1\\nORDER BY 1 DESC\\n\",), kwargs={}).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at_day</th>\n",
       "      <th>subids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-07</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-06</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-04</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_at_day  subids\n",
       "0     2025-09-07     210\n",
       "1     2025-09-06     320\n",
       "2     2025-09-05     313\n",
       "3     2025-09-04     287\n",
       "4     2025-09-03     500"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_data3(\n",
    "    partner_ids,\n",
    "    process_names,\n",
    "    transaction_month_prt,\n",
    "    vertical_ids,\n",
    "    start_date,\n",
    "    end_date\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    query = f\"\"\"\n",
    "SELECT\n",
    "  CAST(SUBSTRING(created_at, 1, 10) AS DATE) AS created_at_day,\n",
    "  COUNT(DISTINCT subid) AS subids\n",
    "FROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\n",
    "WHERE partner_id in ({','.join(map(str, partner_ids))})\n",
    "AND process_name in ({','.join(f\"'{p}'\" for p in process_names)})\n",
    " AND transaction_month_prt >= '{transaction_month_prt}'\n",
    " AND vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    " -- and rn = 1\n",
    "GROUP BY 1\n",
    "ORDER BY 1 DESC\n",
    "\"\"\"\n",
    "    return pd.DataFrame(CachedAthenaApi().execute_fetch(query))\n",
    "\n",
    "df3 = get_data3(\n",
    "    partner_ids=partner_ids,\n",
    "    process_names=process_names,\n",
    "    transaction_month_prt=transaction_month_prt,\n",
    "    vertical_ids=vertical_ids,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date\n",
    ")\n",
    "df3.head()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813a7120",
   "metadata": {},
   "source": [
    "Looks like every day added around 60 +- subids.  looks valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff691cfe084ed9",
   "metadata": {},
   "source": [
    "### Query 4: Hours diff for each field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02ec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-09 11:16:42,361 INFO [ds_logger.py:68] [Cached AthenaApi] Local disk-caching is ENABLED.\n",
      "2025-09-09 11:16:42,362 INFO [ds_logger.py:68] [Cached AthenaApi] Cache miss for execute_fetch(args=(\"\\nWITH raw_enrich AS (\\n  SELECT DISTINCT\\n      subid,\\n      process_name,\\n      CAST(SUBSTRING(transaction_date, 1, 10) AS DATE) AS transaction_day,\\n      transaction_date,\\n      transaction_month_prt,\\n      created_at,\\n      loanamount, creditscore, \\n      rn\\n  FROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\\n  WHERE partner_id in (3158,3178)\\n    AND process_name in ('quicken_rocket_allocated')\\n    AND transaction_month_prt between '2025-09' and '2025-10'\\n    AND vertical_id in ('5fa2b415c91a2010c3432900')\\n    AND loanpurpose = 'Refinance'\\n    AND rn = 1\\n),\\nff AS (\\n  SELECT\\n      cid,\\n      conversion_date,\\n      conversion_timestamp,\\n      clickout_timestamp,\\n      company,\\n      SUM(leads_count) AS leads,\\n      SUM(qualified_leads_count) AS qls,\\n      SUM(sales_count) AS sales\\n  FROM dlk_visitor_funnel_dwh_production.chart_funnel\\n  WHERE partner_id in (3158,3178)\\n    AND dt between '2025-09' and '2025-10'\\n    and clickout_date between cast('2025-09-01' as date) and cast('2025-09-09' as date)\\n    AND vertical_id in ('5fa2b415c91a2010c3432900')\\n    AND traffic_type = 'users'\\n  GROUP BY 1,2,3,4,5\\n  HAVING SUM(leads_count) >= 1\\n),\\ncombined AS (\\n  SELECT\\n      r.subid,\\n      ff.company,\\n      r.transaction_date,\\n      date_format(date_trunc('millisecond', MAX(ff.clickout_timestamp)),\\n        '%Y-%m-%d %H:%i:%s.%f') AS clickout_ts_mt,\\n\\n      date_format( date_trunc( 'millisecond', MIN(CASE WHEN r.creditscore IS NOT NULL THEN\\n              COALESCE(\\n                date_parse(SUBSTRING(r.created_at, 1, 19), '%Y-%m-%dT%H:%i:%s'),\\n                from_iso8601_timestamp(regexp_replace(r.created_at, '([+-]\\\\d2)(\\\\d2)$', '\\\\1:\\\\2'))\\n              )\\n            END\\n          )\\n        ),\\n        '%Y-%m-%d %H:%i:%s.%f'\\n      ) AS creditscore_time,\\n\\n      /* loanamount_time → only when present; parse flexibly, ms-trunc, stringify */\\n      date_format(\\n        date_trunc(\\n          'millisecond',\\n          MIN(\\n            CASE WHEN r.loanamount IS NOT NULL THEN\\n              COALESCE(\\n                date_parse(SUBSTRING(r.created_at, 1, 19), '%Y-%m-%dT%H:%i:%s'),\\n                date_parse(SUBSTRING(r.created_at, 1, 19), '%Y-%m-%d %H:%i:%s'),\\n                from_iso8601_timestamp(regexp_replace(r.created_at, '([+-]\\\\d2)(\\\\d2)$', '\\\\1:\\\\2'))\\n              )\\n            END\\n          )\\n        ),\\n        '%Y-%m-%d %H:%i:%s.%f'\\n      ) AS loanamount_time,\\n      sum(ff.leads) as leads,\\n      sum(ff.qls) as qls,\\n      sum(ff.sales) as sales\\n\\n  FROM raw_enrich r\\n  INNER JOIN ff\\n    ON r.subid = ff.cid\\n   AND r.transaction_day >= ff.conversion_date\\n  GROUP BY r.subid, r.transaction_date, ff.company\\n)\\nSELECT * FROM combined\\n\",), kwargs={}).\n",
      "fetching manifest from s3://aws-athena-query-results-925511037392-us-east-1/Unsaved/2025/09/09/2bbf9383-3848-4bda-952f-30ea1e3b1a1e-manifest.csv\n",
      "2025-09-09 11:16:50,303 INFO [ds_logger.py:68] the function _execute_unload was executed in 7.9402 seconds\n",
      "2025-09-09 11:16:50,888 INFO [ds_logger.py:68] the function _execute_fetch was executed in 8.52493 seconds\n",
      "Added hour difference columns:\n",
      "Columns: ['subid', 'company', 'transaction_date', 'clickout_ts_mt', 'creditscore_time', 'loanamount_time', 'leads', 'qls', 'sales', 'creditscore_diff_hours', 'loanamount_diff_hours']\n",
      "\n",
      "First 5 rows with hour differences:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subid</th>\n",
       "      <th>company</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>clickout_ts_mt</th>\n",
       "      <th>creditscore_time</th>\n",
       "      <th>loanamount_time</th>\n",
       "      <th>leads</th>\n",
       "      <th>qls</th>\n",
       "      <th>sales</th>\n",
       "      <th>creditscore_diff_hours</th>\n",
       "      <th>loanamount_diff_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K1UMk9wWH6</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-09-03 21:00:00+00:00</td>\n",
       "      <td>2025-09-04 13:33:25+00:00</td>\n",
       "      <td>2025-09-05 18:00:51+00:00</td>\n",
       "      <td>2025-09-05 18:00:51+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pcA2j93fnk</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-09-04 21:00:00+00:00</td>\n",
       "      <td>2025-09-05 20:10:46+00:00</td>\n",
       "      <td>2025-09-06 18:00:51+00:00</td>\n",
       "      <td>2025-09-06 18:00:51+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iJJE9l5aYa</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-09-04 21:00:00+00:00</td>\n",
       "      <td>2025-09-05 19:23:26+00:00</td>\n",
       "      <td>2025-09-06 18:00:51+00:00</td>\n",
       "      <td>2025-09-06 18:00:51+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SvAFy00H3Y</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-09-03 21:00:00+00:00</td>\n",
       "      <td>2025-09-05 00:05:19+00:00</td>\n",
       "      <td>2025-09-05 18:00:51+00:00</td>\n",
       "      <td>2025-09-05 18:00:51+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sLVh68kfbe</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-09-04 21:00:00+00:00</td>\n",
       "      <td>2025-09-05 13:01:49+00:00</td>\n",
       "      <td>2025-09-06 18:00:51+00:00</td>\n",
       "      <td>2025-09-06 18:00:51+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subid company          transaction_date            clickout_ts_mt  \\\n",
       "0  K1UMk9wWH6      ni 2025-09-03 21:00:00+00:00 2025-09-04 13:33:25+00:00   \n",
       "1  pcA2j93fnk      ni 2025-09-04 21:00:00+00:00 2025-09-05 20:10:46+00:00   \n",
       "2  iJJE9l5aYa      ni 2025-09-04 21:00:00+00:00 2025-09-05 19:23:26+00:00   \n",
       "3  SvAFy00H3Y      ni 2025-09-03 21:00:00+00:00 2025-09-05 00:05:19+00:00   \n",
       "4  sLVh68kfbe      ni 2025-09-04 21:00:00+00:00 2025-09-05 13:01:49+00:00   \n",
       "\n",
       "           creditscore_time           loanamount_time  leads  qls  sales  \\\n",
       "0 2025-09-05 18:00:51+00:00 2025-09-05 18:00:51+00:00      1    0      0   \n",
       "1 2025-09-06 18:00:51+00:00 2025-09-06 18:00:51+00:00      1    0      1   \n",
       "2 2025-09-06 18:00:51+00:00 2025-09-06 18:00:51+00:00      1    0      0   \n",
       "3 2025-09-05 18:00:51+00:00 2025-09-05 18:00:51+00:00      1    0      0   \n",
       "4 2025-09-06 18:00:51+00:00 2025-09-06 18:00:51+00:00      1    0      0   \n",
       "\n",
       "   creditscore_diff_hours  loanamount_diff_hours  \n",
       "0                    28.0                   28.0  \n",
       "1                    21.0                   21.0  \n",
       "2                    22.0                   22.0  \n",
       "3                    17.0                   17.0  \n",
       "4                    28.0                   28.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data3_combined(\n",
    "    partner_ids,\n",
    "    process_names,\n",
    "    transaction_month_prt, \n",
    "    vertical_ids,\n",
    "    start_date,  # 'YYYY-MM-DD'\n",
    "    end_date     # 'YYYY-MM-DD'\n",
    ") -> pd.DataFrame:\n",
    "    tm_start = start_date[:7]\n",
    "    tm_end = end_date[:7]\n",
    "    if tm_end == tm_start:\n",
    "        year, month = map(int, tm_end.split('-'))\n",
    "        if month == 12:\n",
    "            year += 1\n",
    "            month = 1\n",
    "        else:\n",
    "            month += 1\n",
    "        tm_end = f\"{year:04d}-{month:02d}\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "WITH raw_enrich AS (\n",
    "  SELECT DISTINCT\n",
    "      subid,\n",
    "      process_name,\n",
    "      CAST(SUBSTRING(transaction_date, 1, 10) AS DATE) AS transaction_day,\n",
    "      transaction_date,\n",
    "      transaction_month_prt,\n",
    "      created_at,\n",
    "      {enrichment_cols}\n",
    "      rn\n",
    "  FROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\n",
    "  WHERE partner_id in ({','.join(map(str, partner_ids))})\n",
    "    AND process_name in ({','.join(f\"'{p}'\" for p in process_names)})\n",
    "    AND transaction_month_prt between '{tm_start}' and '{tm_end}'\n",
    "    AND vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    "    {enrich_cond}\n",
    "    AND rn = 1\n",
    "),\n",
    "ff AS (\n",
    "  SELECT\n",
    "      cid,\n",
    "      conversion_date,\n",
    "      conversion_timestamp,\n",
    "      clickout_timestamp,\n",
    "      company,\n",
    "      SUM(leads_count) AS leads,\n",
    "      SUM(qualified_leads_count) AS qls,\n",
    "      SUM(sales_count) AS sales\n",
    "  FROM dlk_visitor_funnel_dwh_production.chart_funnel\n",
    "  WHERE partner_id in ({','.join(map(str, partner_ids))})\n",
    "    AND dt between '{tm_start}' and '{tm_end}'\n",
    "    and clickout_date between cast('{start_date}' as date) and cast('{end_date}' as date)\n",
    "    AND vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    "    AND traffic_type = 'users'\n",
    "  GROUP BY 1,2,3,4,5\n",
    "  HAVING SUM(leads_count) >= 1\n",
    "),\n",
    "combined AS (\n",
    "  SELECT\n",
    "      r.subid,\n",
    "      ff.company,\n",
    "      r.transaction_date,\n",
    "      date_format(date_trunc('millisecond', MAX(ff.clickout_timestamp)),\n",
    "        '%Y-%m-%d %H:%i:%s.%f') AS clickout_ts_mt,\n",
    "\n",
    "      date_format( date_trunc( 'millisecond', MIN(CASE WHEN r.creditscore IS NOT NULL THEN\n",
    "              COALESCE(\n",
    "                date_parse(SUBSTRING(r.created_at, 1, 19), '%Y-%m-%dT%H:%i:%s'),\n",
    "                from_iso8601_timestamp(regexp_replace(r.created_at, '([+-]\\\\d{2})(\\\\d{2})$', '\\\\1:\\\\2'))\n",
    "              )\n",
    "            END\n",
    "          )\n",
    "        ),\n",
    "        '%Y-%m-%d %H:%i:%s.%f'\n",
    "      ) AS creditscore_time,\n",
    "\n",
    "      /* loanamount_time → only when present; parse flexibly, ms-trunc, stringify */\n",
    "      date_format(\n",
    "        date_trunc(\n",
    "          'millisecond',\n",
    "          MIN(\n",
    "            CASE WHEN r.loanamount IS NOT NULL THEN\n",
    "              COALESCE(\n",
    "                date_parse(SUBSTRING(r.created_at, 1, 19), '%Y-%m-%dT%H:%i:%s'),\n",
    "                date_parse(SUBSTRING(r.created_at, 1, 19), '%Y-%m-%d %H:%i:%s'),\n",
    "                from_iso8601_timestamp(regexp_replace(r.created_at, '([+-]\\\\d{2})(\\\\d{2})$', '\\\\1:\\\\2'))\n",
    "              )\n",
    "            END\n",
    "          )\n",
    "        ),\n",
    "        '%Y-%m-%d %H:%i:%s.%f'\n",
    "      ) AS loanamount_time,\n",
    "      sum(ff.leads) as leads,\n",
    "      sum(ff.qls) as qls,\n",
    "      sum(ff.sales) as sales\n",
    "      \n",
    "  FROM raw_enrich r\n",
    "  INNER JOIN ff\n",
    "    ON r.subid = ff.cid\n",
    "   AND r.transaction_day >= ff.conversion_date\n",
    "  GROUP BY r.subid, r.transaction_date, ff.company\n",
    ")\n",
    "SELECT * FROM combined\n",
    "\"\"\"\n",
    "    return pd.DataFrame(CachedAthenaApi().execute_fetch(query))\n",
    "\n",
    "combined_df = get_data3_combined(\n",
    "    partner_ids=partner_ids,\n",
    "    process_names=process_names,\n",
    "    transaction_month_prt=transaction_month_prt,   # ignored for month range; kept for signature symmetry\n",
    "    vertical_ids=vertical_ids,\n",
    "    start_date= start_date,\n",
    "    end_date= end_date\n",
    ")\n",
    "\n",
    "# parse the string timestamps before hours math (your build_hours_summary already does this)\n",
    "for col in [\"clickout_ts_mt\", \"creditscore_time\", \"loanamount_time\", \"transaction_date\"]:\n",
    "    combined_df[col] = pd.to_datetime(combined_df[col], errors=\"coerce\", utc=True)\n",
    "\n",
    "# Add hour difference columns for all columns ending with '_time'\n",
    "def add_hour_diffs(df, reference_col='clickout_ts_mt'):\n",
    "    \"\"\"\n",
    "    Add hour difference columns for all columns ending with '_time'\n",
    "    compared to the reference column (default: clickout_ts_mt)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Find all columns ending with '_time'\n",
    "    time_cols = [col for col in df.columns if col.endswith('_time')]\n",
    "    \n",
    "    for time_col in time_cols:\n",
    "        # Create diff column name\n",
    "        diff_col = time_col.replace('_time', '_diff_hours')\n",
    "        \n",
    "        # Calculate hour difference: (time_col - reference_col) in hours\n",
    "        # Floor the result and ensure non-negative values\n",
    "        time_diff = (df[time_col] - df[reference_col]).dt.total_seconds() / 3600.0\n",
    "        time_diff = np.floor(time_diff)  # Floor like TIMESTAMPDIFF('hour',...)\n",
    "        time_diff = np.maximum(time_diff, 0)  # Ensure non-negative (GREATEST(..., 0))\n",
    "        \n",
    "        # Handle NaN values (when either timestamp is null)\n",
    "        time_diff = np.where(df[time_col].isna() | df[reference_col].isna(), np.nan, time_diff)\n",
    "        \n",
    "        df[diff_col] = time_diff\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to add hour difference columns\n",
    "combined_df = add_hour_diffs(combined_df)\n",
    "\n",
    "print(\"Added hour difference columns:\")\n",
    "print(\"Columns:\", combined_df.columns.tolist())\n",
    "print(\"\\nFirst 5 rows with hour differences:\")\n",
    "combined_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054469d9",
   "metadata": {},
   "source": [
    "### Query 5: aggregated hours diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecd86324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using hour columns: ['creditscore_diff_hours', 'loanamount_diff_hours']\n",
      "Sales column available: True\n",
      "\n",
      "Considering only rows with enrichment values\n",
      "\n",
      "============================================================\n",
      "SUMMARY STATISTICS:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>field_name</th>\n",
       "      <th>avg_hours</th>\n",
       "      <th>median_hours</th>\n",
       "      <th>p80_hours</th>\n",
       "      <th>p90_hours</th>\n",
       "      <th>avg_sales</th>\n",
       "      <th>sum_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ni</td>\n",
       "      <td>CREDITSCORE_DIFF_HOURS</td>\n",
       "      <td>27.250244</td>\n",
       "      <td>24.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.024438</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ni</td>\n",
       "      <td>LOANAMOUNT_DIFF_HOURS</td>\n",
       "      <td>27.250244</td>\n",
       "      <td>24.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.024438</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company              field_name  avg_hours  median_hours  p80_hours  \\\n",
       "0      ni  CREDITSCORE_DIFF_HOURS  27.250244          24.0       29.0   \n",
       "1      ni   LOANAMOUNT_DIFF_HOURS  27.250244          24.0       29.0   \n",
       "\n",
       "   p90_hours  avg_sales  sum_sales  \n",
       "0       42.0   0.024438         25  \n",
       "1       42.0   0.024438         25  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed version with sales stats included, but now removing null rate and filled rows columns, and adding a print statement before stats\n",
    "def compute_field_stats_fixed(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Find all columns ending with '_diff_hours' or '_hours'\n",
    "    hour_cols = [col for col in df.columns if col.endswith('_diff_hours') or col.endswith('_hours')]\n",
    "    \n",
    "    if not hour_cols:\n",
    "        print(\"No hour columns found. Available columns:\", df.columns.tolist())\n",
    "        return pd.DataFrame()\n",
    "    print(f\"Using hour columns: {hour_cols}\")\n",
    "\n",
    "    has_sales = 'sales' in df.columns\n",
    "    print(f\"Sales column available: {has_sales}\")\n",
    "    \n",
    "    long_df = df.melt(\n",
    "        id_vars=[\"subid\", \"company\", \"transaction_date\", \"clickout_ts_mt\"] + ([\"sales\"] if has_sales else []),\n",
    "        value_vars=hour_cols,\n",
    "        var_name=\"field_name\",\n",
    "        value_name=\"hours\"\n",
    "    )\n",
    "    long_df[\"field_name\"] = long_df[\"field_name\"].str.upper()\n",
    "\n",
    "    g = long_df.groupby([\"company\", \"field_name\"], dropna=False)\n",
    "    avg_hours     = g[\"hours\"].mean()\n",
    "    median_hours  = g[\"hours\"].quantile(0.50, interpolation=\"linear\")\n",
    "    p80_hours     = g[\"hours\"].quantile(0.80, interpolation=\"linear\")\n",
    "    p90_hours     = g[\"hours\"].quantile(0.90, interpolation=\"linear\")\n",
    "\n",
    "    # Calculate sales stats if sales column exists\n",
    "    if has_sales:\n",
    "        avg_sales = g[\"sales\"].mean()\n",
    "        sum_sales = g[\"sales\"].sum()\n",
    "        \n",
    "        out = pd.concat(\n",
    "            [avg_hours.rename(\"avg_hours\"),\n",
    "             median_hours.rename(\"median_hours\"),\n",
    "             p80_hours.rename(\"p80_hours\"),\n",
    "             p90_hours.rename(\"p90_hours\"),\n",
    "             avg_sales.rename(\"avg_sales\"),\n",
    "             sum_sales.rename(\"sum_sales\")],\n",
    "            axis=1\n",
    "        ).reset_index()\n",
    "        \n",
    "        out = out[[\"company\",\"field_name\",\"avg_hours\",\"median_hours\",\"p80_hours\",\"p90_hours\",\n",
    "                   \"avg_sales\",\"sum_sales\"]]\\\n",
    "                .sort_values([\"company\",\"field_name\"]).reset_index(drop=True)\n",
    "    else:\n",
    "        out = pd.concat(\n",
    "            [avg_hours.rename(\"avg_hours\"),\n",
    "             median_hours.rename(\"median_hours\"),\n",
    "             p80_hours.rename(\"p80_hours\"),\n",
    "             p90_hours.rename(\"p90_hours\")],\n",
    "            axis=1\n",
    "        ).reset_index()\n",
    "        \n",
    "        out = out[[\"company\",\"field_name\",\"avg_hours\",\"median_hours\",\"p80_hours\",\"p90_hours\"]]\\\n",
    "                .sort_values([\"company\",\"field_name\"]).reset_index(drop=True)\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Use the fixed function\n",
    "field_stats = compute_field_stats_fixed(combined_df)    \n",
    "\n",
    "print(\"\\nConsidering only rows with enrichment values\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"SUMMARY STATISTICS:\")\n",
    "print(\"=\"*60)\n",
    "field_stats.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b15726",
   "metadata": {},
   "source": [
    "### Query 6: for general data for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "00cd4eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-09 11:31:52,288 INFO [ds_logger.py:68] [Cached AthenaApi] Local disk-caching is ENABLED.\n",
      "2025-09-09 11:31:52,289 INFO [ds_logger.py:68] [Cached AthenaApi] Cache miss for execute_fetch(args=(\"\\nWITH raw_enrich AS (\\n  SELECT DISTINCT\\n      subid,\\n      process_name,\\n      transaction_date,\\n      CAST(SUBSTRING(transaction_date, 1, 10) AS DATE) AS transaction_day,\\n      transaction_month_prt,\\n      loanamount, creditscore, \\n      row_number () over (partition by subid order by transaction_date desc) as rnn\\n\\n  FROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\\n  WHERE partner_id in (3158,3178)\\n    AND process_name in ('quicken_rocket_allocated')\\n    AND transaction_month_prt between '2025-01' and '2025-09'\\n    AND vertical_id in ('5fa2b415c91a2010c3432900')\\n    AND rn = 1 \\n    AND loanpurpose = 'Refinance'\\n),\\nff AS (\\n  SELECT\\n      cid,\\n      conversion_date,\\n      conversion_timestamp,\\n      clickout_date,\\n      company,\\n      SUM(leads_count) AS leads,\\n      SUM(qualified_leads_count) AS qls,\\n      SUM(sales_count) AS sales\\n\\n  FROM dlk_visitor_funnel_dwh_production.chart_funnel\\n  WHERE partner_id in (3158,3178)\\n    AND dt between '2025-01' and '2025-09'\\n    AND vertical_id in ('5fa2b415c91a2010c3432900')\\n    AND traffic_type = 'users'\\n  GROUP BY 1,2,3,4,5\\n  HAVING SUM(leads_count) >= 1\\n),\\ncombined AS (\\n  SELECT\\n      coalesce(ff.cid, r.subid) as subid,\\n      ff.company,\\n      r.transaction_date,\\n      clickout_date,\\n      loanamount, creditscore, \\n      sum(ff.leads) as leads,\\n      sum(ff.qls) as qls,\\n      sum(ff.sales) as sales\\n\\n  FROM ff \\n  LEFT JOIN raw_enrich r\\n    ON r.subid = ff.cid\\n   AND r.transaction_day >= ff.conversion_date and rnn = 1 \\n  GROUP BY 1,2,3,4,5,6\\n)\\nSELECT * FROM combined \",), kwargs={}).\n",
      "fetching manifest from s3://aws-athena-query-results-925511037392-us-east-1/Unsaved/2025/09/09/d3d6864d-ddc5-49d4-b8ea-9bc780169987-manifest.csv\n",
      "2025-09-09 11:32:03,563 INFO [ds_logger.py:68] the function _execute_unload was executed in 11.27224 seconds\n",
      "2025-09-09 11:32:05,348 INFO [ds_logger.py:68] the function _execute_fetch was executed in 13.05727 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of              subid company          transaction_date clickout_date  \\\n",
       "0       v8zBooq3KF      ni  2025-08-18T00:00:00+0300    2025-08-18   \n",
       "1       6xoosaFgYb      ni  2025-03-18T00:00:00+0200    2025-03-18   \n",
       "2       MPw62Wmria      ni                      None    2025-02-27   \n",
       "3       dQi5l3uEu0      ni                      None    2025-04-29   \n",
       "4       cFAqUFSxh6      ni                      None    2025-07-04   \n",
       "...            ...     ...                       ...           ...   \n",
       "170757  ygczhros2e      ni  2025-07-24T00:00:00+0300    2025-07-24   \n",
       "170758  iHzHjwbOOq      ni                      None    2025-08-26   \n",
       "170759  2cTshj9SCA      ni                      None    2025-01-15   \n",
       "170760  bEX08o38IE      ni                      None    2025-03-28   \n",
       "170761  cmZDnzJVLb      ni                      None    2025-04-20   \n",
       "\n",
       "        loanamount    creditscore  leads  qls  sales  \n",
       "0         359000.0      Excellent      1    0      0  \n",
       "1              0.0  Below Average      1    0      0  \n",
       "2              NaN           None      1    0      0  \n",
       "3              NaN           None      1    0      0  \n",
       "4              NaN           None      1    0      0  \n",
       "...            ...            ...    ...  ...    ...  \n",
       "170757    148000.0      Excellent      1    0      0  \n",
       "170758         NaN           None      1    0      0  \n",
       "170759         NaN           None      1    0      0  \n",
       "170760         NaN           None      1    0      0  \n",
       "170761         NaN           None      1    0      0  \n",
       "\n",
       "[170762 rows x 9 columns]>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_general_data(\n",
    "    partner_ids,\n",
    "    process_names,\n",
    "    transaction_month_prt,  # not used (we derive months from dates; kept for symmetry)\n",
    "    vertical_ids,\n",
    "    start_date,  # 'YYYY-MM-DD'\n",
    "    end_date     # 'YYYY-MM-DD'\n",
    ") -> pd.DataFrame:\n",
    "    tm_start = start_date[:7]\n",
    "    tm_end   = end_date[:7]\n",
    "\n",
    "    query = f\"\"\"\n",
    "WITH raw_enrich AS (\n",
    "  SELECT DISTINCT\n",
    "      subid,\n",
    "      process_name,\n",
    "      transaction_date,\n",
    "      CAST(SUBSTRING(transaction_date, 1, 10) AS DATE) AS transaction_day,\n",
    "      transaction_month_prt,\n",
    "      {enrichment_cols}\n",
    "      row_number () over (partition by subid order by transaction_date desc) as rnn\n",
    "\n",
    "  FROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\n",
    "  WHERE partner_id in ({','.join(map(str, partner_ids))})\n",
    "    AND process_name in ({','.join(f\"'{p}'\" for p in process_names)})\n",
    "    AND transaction_month_prt between '{tm_start}' and '{tm_end}'\n",
    "    AND vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    "    AND rn = 1 \n",
    "    AND loanpurpose = 'Refinance'\n",
    "),\n",
    "ff AS (\n",
    "  SELECT\n",
    "      cid,\n",
    "      conversion_date,\n",
    "      conversion_timestamp,\n",
    "      clickout_date,\n",
    "      company,\n",
    "      SUM(leads_count) AS leads,\n",
    "      SUM(qualified_leads_count) AS qls,\n",
    "      SUM(sales_count) AS sales\n",
    "\n",
    "  FROM dlk_visitor_funnel_dwh_production.chart_funnel\n",
    "  WHERE partner_id in ({','.join(map(str, partner_ids))})\n",
    "    AND dt between '{tm_start}' and '{tm_end}'\n",
    "    AND vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    "    AND traffic_type = 'users'\n",
    "  GROUP BY 1,2,3,4,5\n",
    "  HAVING SUM(leads_count) >= 1\n",
    "),\n",
    "combined AS (\n",
    "  SELECT\n",
    "      coalesce(ff.cid, r.subid) as subid,\n",
    "      ff.company,\n",
    "      r.transaction_date,\n",
    "      clickout_date,\n",
    "      {enrichment_cols}\n",
    "      sum(ff.leads) as leads,\n",
    "      sum(ff.qls) as qls,\n",
    "      sum(ff.sales) as sales\n",
    "      \n",
    "  FROM ff \n",
    "  LEFT JOIN raw_enrich r\n",
    "    ON r.subid = ff.cid\n",
    "   AND r.transaction_day >= ff.conversion_date and rnn = 1 \n",
    "  GROUP BY 1,2,3,4,5,6\n",
    ")\n",
    "SELECT * FROM combined \"\"\"\n",
    "\n",
    "    return pd.DataFrame(CachedAthenaApi().execute_fetch(query))\n",
    "\n",
    "analysis_df = get_general_data(\n",
    "    partner_ids=partner_ids,\n",
    "    process_names=process_names,\n",
    "    transaction_month_prt= transaction_month_prt,   # ignored for month range; kept for signature symmetry\n",
    "    vertical_ids=vertical_ids,\n",
    "    start_date= start_date,\n",
    "    end_date= end_date\n",
    ")\n",
    "\n",
    "analysis_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3676441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subid</th>\n",
       "      <th>company</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>clickout_date</th>\n",
       "      <th>loanamount</th>\n",
       "      <th>creditscore</th>\n",
       "      <th>leads</th>\n",
       "      <th>qls</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152959</th>\n",
       "      <td>YCD9FmIEyN</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-06-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107397</th>\n",
       "      <td>GekiJI6qrD</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-06-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165535</th>\n",
       "      <td>YbIQfMGdbs</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-07-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112765</th>\n",
       "      <td>vkjHDsyxOM</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-05-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65893</th>\n",
       "      <td>4sGbG5YWlM</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-07-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73415</th>\n",
       "      <td>Dl82pFHyQp</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-06-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39880</th>\n",
       "      <td>1OOGsZdj4P</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-05-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152778</th>\n",
       "      <td>hDGQKJLBhm</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29766</th>\n",
       "      <td>i59JpccfiM</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-01-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143366</th>\n",
       "      <td>y4QCOxx43e</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-08-21T00:00:00+0300</td>\n",
       "      <td>2025-07-06</td>\n",
       "      <td>320000.0</td>\n",
       "      <td>Average</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subid company          transaction_date clickout_date  \\\n",
       "152959  YCD9FmIEyN      ni                      None    2025-06-12   \n",
       "107397  GekiJI6qrD      ni                      None    2025-06-24   \n",
       "165535  YbIQfMGdbs      ni                      None    2025-07-12   \n",
       "112765  vkjHDsyxOM      ni                      None    2025-05-29   \n",
       "65893   4sGbG5YWlM      ni                      None    2025-07-26   \n",
       "73415   Dl82pFHyQp      ni                      None    2025-06-02   \n",
       "39880   1OOGsZdj4P      ni                      None    2025-05-30   \n",
       "152778  hDGQKJLBhm      ni                      None    2025-01-31   \n",
       "29766   i59JpccfiM      ni                      None    2025-01-22   \n",
       "143366  y4QCOxx43e      ni  2025-08-21T00:00:00+0300    2025-07-06   \n",
       "\n",
       "        loanamount creditscore  leads  qls  sales  \n",
       "152959         NaN        None      2    0      0  \n",
       "107397         NaN        None      2    0      0  \n",
       "165535         NaN        None      2    0      0  \n",
       "112765         NaN        None      2    0      0  \n",
       "65893          NaN        None      2    0      0  \n",
       "73415          NaN        None      2    0      0  \n",
       "39880          NaN        None      2    1      0  \n",
       "152778         NaN        None      2    0      0  \n",
       "29766          NaN        None      2    0      0  \n",
       "143366    320000.0     Average      2    0      0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df.sort_values(by='leads', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e666f46",
   "metadata": {},
   "source": [
    "#### Additional Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f69423",
   "metadata": {},
   "source": [
    "##### 1. NULL RATES FOR ENRICHMENT FIELDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "17c741ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. NULL RATES FOR ENRICHMENT FIELDS\n",
      "============================================================\n",
      "Enrichment fields found: ['loanamount', 'creditscore', 'enrich_month']\n",
      "loanamount:\n",
      "  Total rows: 170762\n",
      "  Null rows: 135240\n",
      "  Filled rows: 35522\n",
      "  Null rate: 79.20%\n",
      "----------------------------------------\n",
      "creditscore:\n",
      "  Total rows: 170762\n",
      "  Null rows: 135240\n",
      "  Filled rows: 35522\n",
      "  Null rate: 79.20%\n",
      "----------------------------------------\n",
      "enrich_month:\n",
      "  Total rows: 170762\n",
      "  Null rows: 0\n",
      "  Filled rows: 170762\n",
      "  Null rate: 0.00%\n",
      "----------------------------------------\n",
      "\n",
      "SUMMARY TABLE:\n",
      "          field  null_rate_pct  filled_rows  total_rows\n",
      "0    loanamount           79.2        35522      170762\n",
      "1   creditscore           79.2        35522      170762\n",
      "2  enrich_month            0.0       170762      170762\n"
     ]
    }
   ],
   "source": [
    "# 1. NULL RATES FOR ENRICHMENT FIELDS\n",
    "print(\"=\"*60)\n",
    "print(\"1. NULL RATES FOR ENRICHMENT FIELDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "enrichment_fields = [col for col in analysis_df.columns if col not in ['subid', 'company', 'transaction_date', 'leads', 'qls', 'sales','l2s', 'clickout_date']]\n",
    "print(f\"Enrichment fields found: {enrichment_fields}\")\n",
    "\n",
    "null_rates = []\n",
    "for field in enrichment_fields:\n",
    "    total_rows = len(analysis_df)\n",
    "    null_rows = analysis_df[field].isnull().sum()\n",
    "    null_rate = (null_rows / total_rows) * 100\n",
    "    \n",
    "    null_rates.append({\n",
    "        'field': field,\n",
    "        'total_rows': total_rows,\n",
    "        'null_rows': null_rows,\n",
    "        'null_rate_pct': null_rate,\n",
    "        'filled_rows': total_rows - null_rows\n",
    "    })\n",
    "    \n",
    "    print(f\"{field}:\")\n",
    "    print(f\"  Total rows: {total_rows}\")\n",
    "    print(f\"  Null rows: {null_rows}\")\n",
    "    print(f\"  Filled rows: {total_rows - null_rows}\")\n",
    "    print(f\"  Null rate: {null_rate:.2f}%\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Create summary DataFrame\n",
    "null_summary = pd.DataFrame(null_rates)\n",
    "print(\"\\nSUMMARY TABLE:\")\n",
    "print(null_summary[['field', 'null_rate_pct', 'filled_rows', 'total_rows']].round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f543f258",
   "metadata": {},
   "source": [
    "Per Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bc51a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per month analysis of enrichment columns (non-null counts and null rates):\n",
      "enrich_month  total_records  creditscore_non_null_count  creditscore_null_count  loanamount_non_null_count  loanamount_null_count  creditscore_null_rate  loanamount_null_rate\n",
      "     2025-01          29856                        6808                   23048                       6808                  23048               0.771972              0.771972\n",
      "     2025-02          23428                        5373                   18055                       5373                  18055               0.770659              0.770659\n",
      "     2025-03          23203                        4860                   18343                       4860                  18343               0.790544              0.790544\n",
      "     2025-04          20779                        4403                   16376                       4403                  16376               0.788103              0.788103\n",
      "     2025-05          17910                        3458                   14452                       3458                  14452               0.806924              0.806924\n",
      "     2025-06          16140                        2840                   13300                       2840                  13300               0.824040              0.824040\n",
      "     2025-07          17433                        3103                   14330                       3103                  14330               0.822004              0.822004\n",
      "     2025-08          22012                        4677                   17335                       4677                  17335               0.787525              0.787525\n",
      "     2025-09              1                           0                       1                          0                      1               1.000000              1.000000\n"
     ]
    }
   ],
   "source": [
    "# Per month analysis of how many values exist for enrichment columns, and null rate\n",
    "# Use clickout_date (or clickout_ts_mt/clickout_timestamp) for month extraction\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define which columns are considered \"enrichment columns\"\n",
    "enrichment_columns = ['creditscore', 'loanamount']\n",
    "\n",
    "# Choose the correct timestamp column for month extraction\n",
    "timestamp_col = None\n",
    "for col in ['clickout_ts_mt', 'clickout_date', 'clickout_timestamp']:\n",
    "    if col in analysis_df.columns:\n",
    "        timestamp_col = col\n",
    "        break\n",
    "\n",
    "if timestamp_col is None:\n",
    "    raise ValueError(\"No clickout timestamp column found in analysis_df\")\n",
    "\n",
    "# Ensure the timestamp column is in datetime format\n",
    "if not pd.api.types.is_datetime64_any_dtype(analysis_df[timestamp_col]):\n",
    "    analysis_df[timestamp_col] = pd.to_datetime(analysis_df[timestamp_col], errors='coerce')\n",
    "\n",
    "analysis_df['enrich_month'] = analysis_df[timestamp_col].dt.to_period('M')\n",
    "\n",
    "# Prepare per-month stats for each enrichment column\n",
    "# The correct null rate is null_count / total_records (where total_records is the number of rows for that month)\n",
    "month_counts = analysis_df.groupby('enrich_month').size().rename('total_records').reset_index()\n",
    "\n",
    "enrichment_stats = [month_counts]\n",
    "for col in enrichment_columns:\n",
    "    per_month = (\n",
    "        analysis_df\n",
    "        .groupby('enrich_month')[col]\n",
    "        .agg(\n",
    "            **{\n",
    "                f'{col}_non_null_count': lambda x: x.notnull().sum(),\n",
    "                f'{col}_null_count': lambda x: x.isnull().sum()\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    enrichment_stats.append(per_month)\n",
    "\n",
    "# Merge all stats on enrich_month\n",
    "from functools import reduce\n",
    "enrichment_per_month = reduce(lambda left, right: pd.merge(left, right, on='enrich_month', how='outer'), enrichment_stats)\n",
    "\n",
    "# Calculate null rates for each enrichment column\n",
    "for col in enrichment_columns:\n",
    "    enrichment_per_month[f'{col}_null_rate'] = enrichment_per_month[f'{col}_null_count'] / enrichment_per_month['total_records']\n",
    "\n",
    "# Display the result\n",
    "print(\"\\nPer month analysis of enrichment columns (non-null counts and null rates):\")\n",
    "print(enrichment_per_month.sort_values('enrich_month').to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32abd32",
   "metadata": {},
   "source": [
    "Rows where both creditscore & loanamount are nulls (For Rocket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24e7c419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with null creditscore: 135240\n",
      "Whenever creditscore is null, loanamount is also null.\n",
      "Whenever loanamount is null, creditscore is also null.\n"
     ]
    }
   ],
   "source": [
    "# Count of rows with null creditscore\n",
    "null_creditscore_count = analysis_df['creditscore'].isnull().sum()\n",
    "print(f\"Number of rows with null creditscore: {null_creditscore_count}\")\n",
    "\n",
    "# Check if whenever creditscore is null, loanamount is also null\n",
    "cs_null_la_notnull = analysis_df[analysis_df['creditscore'].isnull() & analysis_df['loanamount'].notnull()]\n",
    "if cs_null_la_notnull.empty:\n",
    "    print(\"Whenever creditscore is null, loanamount is also null.\")\n",
    "else:\n",
    "    print(f\"There are {len(cs_null_la_notnull)} rows where creditscore is null but loanamount is NOT null.\")\n",
    "\n",
    "# Check if whenever loanamount is null, creditscore is also null\n",
    "la_null_cs_notnull = analysis_df[analysis_df['loanamount'].isnull() & analysis_df['creditscore'].notnull()]\n",
    "if la_null_cs_notnull.empty:\n",
    "    print(\"Whenever loanamount is null, creditscore is also null.\")\n",
    "else:\n",
    "    print(f\"There are {len(la_null_cs_notnull)} rows where loanamount is null but creditscore is NOT null.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd62f2",
   "metadata": {},
   "source": [
    "##### 2. SALES AND QLS ANALYSIS BY ENRICHMENT FIELD VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "360985e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "2. SALES AND QLS ANALYSIS BY ENRICHMENT FIELD VALUES\n",
      "============================================================\n",
      "\n",
      "LOANAMOUNT:\n",
      "----------------------------------------\n",
      "Top 10 values by total leads:\n",
      "loanamount  leads_count  total_leads  total_qls  avg_qls_per_record  total_sales  avg_sales_per_record  avg_l2s_rate\n",
      "      NULL       135240       135268       2497              0.0185           10                0.0001        0.0001\n",
      "  100000.0          998          998          0              0.0000           12                0.0120        0.0120\n",
      "  150000.0          894          894          0              0.0000           12                0.0134        0.0134\n",
      "   50000.0          889          889          0              0.0000           21                0.0236        0.0236\n",
      "  200000.0          618          618          0              0.0000            4                0.0065        0.0065\n",
      "  300000.0          473          473          0              0.0000            7                0.0148        0.0148\n",
      "  250000.0          446          446          0              0.0000            3                0.0067        0.0067\n",
      "   60000.0          397          397          0              0.0000            7                0.0176        0.0176\n",
      "  350000.0          368          368          0              0.0000            3                0.0082        0.0082\n",
      "   10000.0          332          332          0              0.0000            0                0.0000        0.0000\n",
      "\n",
      "Overall summary for loanamount:\n",
      "  Unique values: 4834\n",
      "  Total leads: 170797\n",
      "  Total QLS: 2497\n",
      "  Total sales: 636\n",
      "  Overall L2S rate: 0.37%\n",
      "============================================================\n",
      "\n",
      "CREDITSCORE:\n",
      "----------------------------------------\n",
      "Top 10 values by total leads:\n",
      "  creditscore  leads_count  total_leads  total_qls  avg_qls_per_record  total_sales  avg_sales_per_record  avg_l2s_rate\n",
      "         NULL       135240       135268       2497              0.0185           10                0.0001        0.0001\n",
      "    Excellent        17820        17822          0              0.0000          347                0.0195        0.0195\n",
      "         Good         8018         8020          0              0.0000          188                0.0234        0.0234\n",
      "      Average         4550         4552          0              0.0000           70                0.0154        0.0154\n",
      "Below Average         3251         3252          0              0.0000           19                0.0058        0.0058\n",
      "         Poor         1867         1867          0              0.0000            2                0.0011        0.0011\n",
      "      Unknown           16           16          0              0.0000            0                0.0000        0.0000\n",
      "\n",
      "Overall summary for creditscore:\n",
      "  Unique values: 7\n",
      "  Total leads: 170797\n",
      "  Total QLS: 2497\n",
      "  Total sales: 636\n",
      "  Overall L2S rate: 0.37%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 2. SALES AND QLS MEAN AND SUMS PER ENRICHMENT FIELD VALUES\n",
    "print(\"=\"*60)\n",
    "print(\"2. SALES AND QLS ANALYSIS BY ENRICHMENT FIELD VALUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate L2S (Leads to Sales) conversion rate\n",
    "analysis_df['l2s'] = analysis_df['sales'] / analysis_df['leads'].replace(0, np.nan)\n",
    "\n",
    "for field in enrichment_fields:\n",
    "    print(f\"\\n{field.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Treat nulls as a separate category for grouping\n",
    "    field_stats = (\n",
    "        analysis_df\n",
    "        .copy()\n",
    "        .assign(**{field: analysis_df[field].where(analysis_df[field].notnull(), 'NULL')})\n",
    "        .groupby(field)\n",
    "        .agg({\n",
    "            'leads': ['count', 'sum'],\n",
    "            'qls': ['sum', 'mean'],\n",
    "            'sales': ['sum', 'mean'],\n",
    "            'l2s': 'mean'\n",
    "        })\n",
    "        .round(4)\n",
    "    )\n",
    "    \n",
    "    # Flatten column names\n",
    "    field_stats.columns = ['_'.join(col).strip() for col in field_stats.columns]\n",
    "    field_stats = field_stats.reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    field_stats = field_stats.rename(columns={\n",
    "        f'{field}': field,\n",
    "        'leads_count': 'leads_count',\n",
    "        'leads_sum': 'total_leads',\n",
    "        'qls_sum': 'total_qls',\n",
    "        'qls_mean': 'avg_qls_per_record',\n",
    "        'sales_sum': 'total_sales',\n",
    "        'sales_mean': 'avg_sales_per_record',\n",
    "        'l2s_mean': 'avg_l2s_rate'\n",
    "    })\n",
    "    \n",
    "    # Sort by total leads descending\n",
    "    field_stats = field_stats.sort_values('total_leads', ascending=False)\n",
    "    \n",
    "    print(f\"Top 10 values by total leads:\")\n",
    "    print(field_stats.head(10).to_string(index=False))\n",
    "    \n",
    "    # For overall summary, count nulls as a unique value\n",
    "    unique_values = analysis_df[field].nunique(dropna=True) + analysis_df[field].isnull().any()\n",
    "    print(f\"\\nOverall summary for {field}:\")\n",
    "    print(f\"  Unique values: {unique_values}\")\n",
    "    print(f\"  Total leads: {analysis_df['leads'].sum()}\")\n",
    "    print(f\"  Total QLS: {analysis_df['qls'].sum()}\")\n",
    "    print(f\"  Total sales: {analysis_df['sales'].sum()}\")\n",
    "    print(f\"  Overall L2S rate: {(analysis_df['sales'].sum() / analysis_df['leads'].sum() * 100):.2f}%\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add67128",
   "metadata": {},
   "source": [
    "##### 2.1. Loanamount Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c6826059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOANAMOUNT BINNED ANALYSIS\n",
      "============================================================\n",
      "Top 10 bins by total leads:\n",
      "        loanamount_bin  leads_count  total_leads  total_qls  avg_qls_per_record  total_sales  avg_sales_per_record  avg_l2s_rate\n",
      "                  NULL       135240       135268       2497            0.018463           10              0.000074      0.000074\n",
      "     (-0.001, 85000.0]         5922         5924          0            0.000000          101              0.017055      0.017055\n",
      "   (85000.0, 161000.0]         5932         5933          0            0.000000          126              0.021241      0.021241\n",
      "  (161000.0, 245000.0]         5924         5925          0            0.000000          108              0.018231      0.018231\n",
      "  (245000.0, 339000.0]         5910         5911          0            0.000000          116              0.019628      0.019628\n",
      "  (339000.0, 490000.0]         5933         5935          0            0.000000          114              0.019215      0.019215\n",
      "(490000.0, 19999998.0]         5901         5901          0            0.000000           61              0.010337      0.010337\n",
      "\n",
      "Overall summary for loanamount (binned):\n",
      "  Unique bins: 7\n",
      "  Total leads: 170797\n",
      "  Total QLS: 2497\n",
      "  Total sales: 636\n",
      "  Overall L2S rate: 0.37%\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/19/tgcx3l5d53s090pxf4c256w80000gn/T/ipykernel_79904/222677031.py:37: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'NULL' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loan_df.loc[loan_null_mask, 'loanamount_bin'] = 'NULL'\n"
     ]
    }
   ],
   "source": [
    "# Bin loanamount into up to 7 bins (including 1 for nulls), using qcut for equal leads per bin\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOANAMOUNT BINNED ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare a copy to avoid modifying original\n",
    "loan_df = analysis_df.copy()\n",
    "\n",
    "# Separate nulls\n",
    "loan_null_mask = loan_df['loanamount'].isnull()\n",
    "loan_nonnull = loan_df.loc[~loan_null_mask].copy()\n",
    "\n",
    "# We want up to 6 bins for non-nulls, 1 for nulls (total 7)\n",
    "n_bins = 6\n",
    "# If there are fewer unique non-null values than bins, reduce bins\n",
    "unique_nonnull = loan_nonnull['loanamount'].nunique()\n",
    "if unique_nonnull < n_bins:\n",
    "    n_bins = unique_nonnull\n",
    "\n",
    "# Use qcut to bin by equal number of leads (rows) per bin\n",
    "if n_bins > 1:\n",
    "    # qcut may fail if there are too many duplicate values, so handle that\n",
    "    try:\n",
    "        loan_nonnull['loanamount_bin'] = pd.qcut(loan_nonnull['loanamount'], q=n_bins, duplicates='drop')\n",
    "    except ValueError:\n",
    "        # fallback: use cut with equal-width bins\n",
    "        loan_nonnull['loanamount_bin'] = pd.cut(loan_nonnull['loanamount'], bins=n_bins)\n",
    "else:\n",
    "    loan_nonnull['loanamount_bin'] = loan_nonnull['loanamount']\n",
    "\n",
    "# Assign 'NULL' bin to nulls\n",
    "loan_df['loanamount_bin'] = np.nan\n",
    "loan_df.loc[loan_null_mask, 'loanamount_bin'] = 'NULL'\n",
    "loan_df.loc[~loan_null_mask, 'loanamount_bin'] = loan_nonnull['loanamount_bin'].astype(str).values\n",
    "\n",
    "# Group and aggregate\n",
    "loanamount_stats = (\n",
    "    loan_df\n",
    "    .groupby('loanamount_bin')\n",
    "    .agg(\n",
    "        leads_count=('leads', 'count'),\n",
    "        total_leads=('leads', 'sum'),\n",
    "        total_qls=('qls', 'sum'),\n",
    "        avg_qls_per_record=('qls', 'mean'),\n",
    "        total_sales=('sales', 'sum'),\n",
    "        avg_sales_per_record=('sales', 'mean'),\n",
    "        avg_l2s_rate=('l2s', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Sort bins: NULL first, then by bin order\n",
    "def sort_key(x):\n",
    "    if x == 'NULL':\n",
    "        return -1\n",
    "    # Try to extract left edge of interval for sorting\n",
    "    try:\n",
    "        if x.startswith('(') or x.startswith('['):\n",
    "            return float(x.split(',')[0].replace('(','').replace('[',''))\n",
    "        else:\n",
    "            return float(x)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "loanamount_stats = loanamount_stats.sort_values(by='loanamount_bin', key=lambda col: col.map(sort_key))\n",
    "\n",
    "print(\"Top 10 bins by total leads:\")\n",
    "print(loanamount_stats.head(10).to_string(index=False))\n",
    "\n",
    "# Overall summary\n",
    "unique_bins = loanamount_stats['loanamount_bin'].nunique()\n",
    "print(f\"\\nOverall summary for loanamount (binned):\")\n",
    "print(f\"  Unique bins: {unique_bins}\")\n",
    "print(f\"  Total leads: {loan_df['leads'].sum()}\")\n",
    "print(f\"  Total QLS: {loan_df['qls'].sum()}\")\n",
    "print(f\"  Total sales: {loan_df['sales'].sum()}\")\n",
    "print(f\"  Overall L2S rate: {(loan_df['sales'].sum() / loan_df['leads'].sum() * 100):.2f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97c628c",
   "metadata": {},
   "source": [
    "##### 3. Per Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb40fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "3. PER COMPANY L2S ANALYSIS\n",
      "============================================================\n",
      "Company Performance Summary:\n",
      "         total_records  total_leads  total_qls  total_sales  avg_l2s_rate  l2s_overall  qls_rate  sales_rate\n",
      "company                                                                                                     \n",
      "ni              170762       170797       2497          636        0.0037         0.37      1.46        0.37\n",
      "\n",
      "Overall Summary:\n",
      "  Total companies: 1\n",
      "  Total records: 170762\n",
      "  Total leads: 170797\n",
      "  Total QLS: 2497\n",
      "  Total sales: 636\n",
      "  Overall L2S rate: 0.37%\n"
     ]
    }
   ],
   "source": [
    "# 3. PER COMPANY L2S ANALYSIS\n",
    "print(\"=\"*60)\n",
    "print(\"3. PER COMPANY L2S ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Company-level analysis\n",
    "company_stats = analysis_df.groupby('company').agg({\n",
    "    'subid': 'count',\n",
    "    'leads': 'sum',\n",
    "    'qls': 'sum', \n",
    "    'sales': 'sum',\n",
    "    'l2s': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "company_stats = company_stats.rename(columns={\n",
    "    'subid': 'total_records',\n",
    "    'leads': 'total_leads',\n",
    "    'qls': 'total_qls',\n",
    "    'sales': 'total_sales',\n",
    "    'l2s': 'avg_l2s_rate'\n",
    "})\n",
    "\n",
    "# Calculate additional metrics\n",
    "company_stats['l2s_overall'] = (company_stats['total_sales'] / company_stats['total_leads'] * 100).round(2)\n",
    "company_stats['qls_rate'] = (company_stats['total_qls'] / company_stats['total_leads'] * 100).round(2)\n",
    "company_stats['sales_rate'] = (company_stats['total_sales'] / company_stats['total_leads'] * 100).round(2)\n",
    "\n",
    "# Sort by total sales\n",
    "company_stats = company_stats.sort_values('total_sales', ascending=False)\n",
    "\n",
    "print(\"Company Performance Summary:\")\n",
    "print(company_stats.to_string())\n",
    "\n",
    "print(f\"\\nOverall Summary:\")\n",
    "print(f\"  Total companies: {len(company_stats)}\")\n",
    "print(f\"  Total records: {company_stats['total_records'].sum()}\")\n",
    "print(f\"  Total leads: {company_stats['total_leads'].sum()}\")\n",
    "print(f\"  Total QLS: {company_stats['total_qls'].sum()}\")\n",
    "print(f\"  Total sales: {company_stats['total_sales'].sum()}\")\n",
    "print(f\"  Overall L2S rate: {(company_stats['total_sales'].sum() / company_stats['total_leads'].sum() * 100):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4bad1b",
   "metadata": {},
   "source": [
    "all rows has company value (220010 leads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bffbf2",
   "metadata": {},
   "source": [
    "##### 4. CATEGORICAL ANALYSIS + CORRELATION ANALYSIS AND CATEGORICAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bf448677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4. CORRELATION ANALYSIS AND CATEGORICAL ANALYSIS\n",
      "============================================================\n",
      "\n",
      "LOANAMOUNT:\n",
      "----------------------------------------\n",
      "Field type: Numeric\n",
      "\n",
      "CORRELATION WITH SALES:\n",
      "  loanamount vs sales: -0.0165\n",
      "  loanamount vs leads: -0.0027\n",
      "  loanamount vs qls: No correlation (constant values or insufficient data)\n",
      "  loanamount vs l2s: -0.0165\n",
      "============================================================\n",
      "\n",
      "CREDITSCORE:\n",
      "----------------------------------------\n",
      "Field type: Categorical\n",
      "\n",
      "TOP 10 CATEGORIES BY SALES:\n",
      "Category             Leads      QLS        Sales      L2S Rate  \n",
      "----------------------------------------------------------------------\n",
      "Excellent            17822      0          347        1.95      %\n",
      "Good                 8020       0          188        2.34      %\n",
      "Average              4552       0          70         1.54      %\n",
      "Below Average        3252       0          19         0.58      %\n",
      "Poor                 1867       0          2          0.11      %\n",
      "Unknown              16         0          0          0.00      %\n",
      "\n",
      "Total categories: 6\n",
      "Categories with sales > 0: 5\n",
      "\n",
      "Category Performance Insights:\n",
      "  Best L2S rate: 2.34%\n",
      "  Worst L2S rate: 0.00%\n",
      "  Average L2S rate: 1.09%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 5. CORRELATION ANALYSIS AND CATEGORICAL ANALYSIS (FIXED VERSION)\n",
    "print(\"=\"*60)\n",
    "print(\"4. CORRELATION ANALYSIS AND CATEGORICAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Function to detect if field is numeric\n",
    "def is_numeric_field(series):\n",
    "    try:\n",
    "        pd.to_numeric(series, errors='raise')\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Function to safely calculate correlation\n",
    "def safe_correlation(series1, series2):\n",
    "    try:\n",
    "        # Remove null values\n",
    "        data = pd.DataFrame({'x': series1, 'y': series2}).dropna()\n",
    "        if len(data) < 2:\n",
    "            return np.nan\n",
    "        # Check if either series has zero variance\n",
    "        if data['x'].std() == 0 or data['y'].std() == 0:\n",
    "            return np.nan\n",
    "        return data['x'].corr(data['y'])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "for field in enrichment_fields:\n",
    "    print(f\"\\n{field.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check if field is numeric\n",
    "    is_numeric = is_numeric_field(analysis_df[field])\n",
    "    print(f\"Field type: {'Numeric' if is_numeric else 'Categorical'}\")\n",
    "    \n",
    "    if is_numeric:\n",
    "        # Correlation analysis for numeric fields\n",
    "        print(\"\\nCORRELATION WITH SALES:\")\n",
    "        # Remove null values for correlation\n",
    "        numeric_data = analysis_df[[field, 'sales', 'leads', 'qls', 'l2s']].dropna()\n",
    "        \n",
    "        if len(numeric_data) > 1:\n",
    "            correlations = {\n",
    "                'sales': safe_correlation(numeric_data[field], numeric_data['sales']),\n",
    "                'leads': safe_correlation(numeric_data[field], numeric_data['leads']),\n",
    "                'qls': safe_correlation(numeric_data[field], numeric_data['qls']),\n",
    "                'l2s': safe_correlation(numeric_data[field], numeric_data['l2s'])\n",
    "            }\n",
    "            \n",
    "            for metric, corr in correlations.items():\n",
    "                if np.isnan(corr):\n",
    "                    print(f\"  {field} vs {metric}: No correlation (constant values or insufficient data)\")\n",
    "                else:\n",
    "                    print(f\"  {field} vs {metric}: {corr:.4f}\")\n",
    "        else:\n",
    "            print(\"  Not enough data for correlation analysis\")\n",
    "            \n",
    "    else:\n",
    "        # Categorical analysis\n",
    "        print(\"\\nTOP 10 CATEGORIES BY SALES:\")\n",
    "        cat_analysis = analysis_df.groupby(field).agg({\n",
    "            'leads': 'sum',\n",
    "            'qls': 'sum',\n",
    "            'sales': 'sum',\n",
    "            'l2s': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Calculate L2S rate safely (avoid division by zero)\n",
    "        cat_analysis['l2s_rate'] = np.where(\n",
    "            cat_analysis['leads'] > 0,\n",
    "            (cat_analysis['sales'] / cat_analysis['leads'] * 100).round(2),\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Sort by sales\n",
    "        cat_analysis = cat_analysis.sort_values('sales', ascending=False)\n",
    "        \n",
    "        # Show top 10\n",
    "        top_10 = cat_analysis.head(10)\n",
    "        print(f\"{'Category':<20} {'Leads':<10} {'QLS':<10} {'Sales':<10} {'L2S Rate':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "        for category, row in top_10.iterrows():\n",
    "            print(f\"{str(category)[:18]:<20} {row['leads']:<10.0f} {row['qls']:<10.0f} {row['sales']:<10.0f} {row['l2s_rate']:<10.2f}%\")\n",
    "        # Yes, the correlation is calculated after dropping rows with nulls in the field being analyzed and the target metric.\n",
    "        # See: numeric_data = analysis_df[[field, 'sales', 'leads', 'qls', 'l2s']].dropna()\n",
    "        # This means only rows where both the field and the metric (e.g., sales) are not null are included in the correlation calculation.\n",
    "        print(f\"\\nTotal categories: {len(cat_analysis)}\")\n",
    "        print(f\"Categories with sales > 0: {len(cat_analysis[cat_analysis['sales'] > 0])}\")\n",
    "        \n",
    "        # Additional insights for categorical data\n",
    "        print(f\"\\nCategory Performance Insights:\")\n",
    "        print(f\"  Best L2S rate: {cat_analysis[cat_analysis['leads'] > 0]['l2s_rate'].max():.2f}%\")\n",
    "        print(f\"  Worst L2S rate: {cat_analysis[cat_analysis['leads'] > 0]['l2s_rate'].min():.2f}%\")\n",
    "        print(f\"  Average L2S rate: {cat_analysis[cat_analysis['leads'] > 0]['l2s_rate'].mean():.2f}%\")\n",
    "    \n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0edbb24",
   "metadata": {},
   "source": [
    "##### 5. INVESTIGATION: Leads vs Records Discrepancy (rows with more than 1 lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8ce670b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INVESTIGATION: Leads vs Records Discrepancy\n",
      "============================================================\n",
      "Total records: 170762\n",
      "Total leads: 170797\n",
      "Average leads per record: 1.0002\n",
      "\n",
      "Leads distribution:\n",
      "leads\n",
      "1    170727\n",
      "2        35\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Records with leads > 1:\n",
      "Count: 35\n",
      "Percentage: 0.02%\n",
      "\n",
      "Top records with highest leads:\n",
      "             subid  leads  qls  sales\n",
      "10828   KGfE1FMa1A      2    1      0\n",
      "143366  y4QCOxx43e      2    0      0\n",
      "112765  vkjHDsyxOM      2    0      0\n",
      "114048  8sivNysjEN      2    0      0\n",
      "115401  IKdwaEMaJD      2    1      0\n",
      "117247  5dBAiZv3XM      2    0      0\n",
      "133885  BHDJjWFZYg      2    0      0\n",
      "134743  4XI2mjMuPH      2    0      0\n",
      "150485  OTBjDRULV0      2    0      0\n",
      "110555  aderKZPPJM      2    0      0\n",
      "\n",
      "Leads statistics:\n",
      "Min: 1\n",
      "Max: 2\n",
      "Median: 1.0\n",
      "Std: 0.01\n",
      "\n",
      "Records with 0 leads: 0\n"
     ]
    }
   ],
   "source": [
    "# INVESTIGATION: Leads vs Records Discrepancy\n",
    "print(\"=\"*60)\n",
    "print(\"INVESTIGATION: Leads vs Records Discrepancy\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Total records: {len(analysis_df)}\")\n",
    "print(f\"Total leads: {analysis_df['leads'].sum()}\")\n",
    "print(f\"Average leads per record: {analysis_df['leads'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nLeads distribution:\")\n",
    "print(analysis_df['leads'].value_counts().sort_index().head(20))\n",
    "\n",
    "print(f\"\\nRecords with leads > 1:\")\n",
    "leads_gt_1 = analysis_df[analysis_df['leads'] > 1]\n",
    "print(f\"Count: {len(leads_gt_1)}\")\n",
    "print(f\"Percentage: {len(leads_gt_1) / len(analysis_df) * 100:.2f}%\")\n",
    "\n",
    "if len(leads_gt_1) > 0:\n",
    "    print(f\"\\nTop records with highest leads:\")\n",
    "    print(leads_gt_1[['subid', 'leads', 'qls', 'sales']].sort_values('leads', ascending=False).head(10))\n",
    "\n",
    "print(f\"\\nLeads statistics:\")\n",
    "print(f\"Min: {analysis_df['leads'].min()}\")\n",
    "print(f\"Max: {analysis_df['leads'].max()}\")\n",
    "print(f\"Median: {analysis_df['leads'].median()}\")\n",
    "print(f\"Std: {analysis_df['leads'].std():.2f}\")\n",
    "\n",
    "# Check if there are any records with 0 leads\n",
    "zero_leads = analysis_df[analysis_df['leads'] == 0]\n",
    "print(f\"\\nRecords with 0 leads: {len(zero_leads)}\")\n",
    "if len(zero_leads) > 0:\n",
    "    print(\"This might explain the discrepancy - some records have 0 leads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5c57d1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subid', 'company', 'transaction_date', 'clickout_date', 'loanamount',\n",
       "       'creditscore', 'leads', 'qls', 'sales', 'l2s'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7db33",
   "metadata": {},
   "source": [
    "### Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb2b68",
   "metadata": {},
   "source": [
    "- The Rocket Refinance Allocated is a small portion of the leads. only 20% of the leads (So 80% null rate for the enrichment fields).\n",
    "- Null rows has almost 0 sales but all QLs. \n",
    "- Creditscore is a good predictor for L2S\n",
    "- Loanamount is not a good predictor (20% corr and doesnt look good .. Loanamount bins has pretty equal L2S values).\n",
    "- Most users has 1 lead (from Jan only 35 users had 2 leads).\n",
    "- Seems like the columns takes 20-30 hours to be filled but we can't verify it because of data structure. \n",
    "- All rows has rn = 1 (enrichment flattern table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c53d83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analysis_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43manalysis_df\u001b[49m.leads.value_counts())\n",
      "\u001b[31mNameError\u001b[39m: name 'analysis_df' is not defined"
     ]
    }
   ],
   "source": [
    "print('thank you and we will meet again in the next partenr check :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6af08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checking-new-partners-enrichment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
