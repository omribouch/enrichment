{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537a8c4591fc1b2a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:46:59.074241Z",
     "start_time": "2025-09-07T14:46:59.070720Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from math import isnan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from ds_aws_services.athena_api  import cachedAthenaApi\n",
    "from ds_aws_services import CachedAthenaApi\n",
    "import pandas as pd\n",
    "# from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "os.environ['disk_caching'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2ebad395a59b2",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e0932fbb2c306",
   "metadata": {},
   "source": [
    "### Params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aa3bd542ed2d3eb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:40:45.098660Z",
     "start_time": "2025-09-07T15:40:45.096239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Biz2credit BI\n",
    "partner_ids = [13589]\n",
    "process_names = [\"bi_biz2credit_lead\"]\n",
    "transaction_month_prt = \"2025-01\"\n",
    "vertical_ids = [\"64e33e7be3cbc4ce1041a30f\"]\n",
    "start_date = \"2025-08-01\"\n",
    "end_date = \"2025-09-06\"\n",
    "transaction_month_prt_start = \"2025-08\"\n",
    "transaction_month_prt_end = \"2025-10\"\n",
    "\n",
    "enrichment_cols = \"age_of_business_months, application_annual_revenue, business_legal_structure, loan_purpose, industry, sub_industry, users_prob_sale, \"\n",
    "max_enrichment_cols = \"MAX(age_of_business_months) AS age_of_business_months, MAX(application_annual_revenue) AS application_annual_revenue, max(business_legal_structure) as business_legal_structure, max(loan_purpose) as loan_purpose, max(industry) as industry, max(sub_industry) as sub_industry, max(users_prob_sale) as users_prob_sale \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bfb5a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rocket \n",
    "partner_ids = [3158,3178]\n",
    "process_names = [\"quicken_rocket_allocated\"]\n",
    "transaction_month_prt = \"2025-01\"\n",
    "vertical_ids = [\"5fa2b415c91a2010c3432900\"]\n",
    "start_date = \"2025-08-01\"\n",
    "end_date = \"2025-09-07\"\n",
    "transaction_month_prt_start = \"2025-08\"\n",
    "transaction_month_prt_end = \"2025-10\"\n",
    "\n",
    "enrichment_cols = \"loanamount, creditscore, \"\n",
    "max_enrichment_cols = \"MAX(f.loanamount)  AS loan_amount, MAX(f.creditscore) AS credit_score \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8ada34f7b78",
   "metadata": {},
   "source": [
    "### Query 1 :  Checking RN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b82b6fa10dd4e0da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:47:05.720283Z",
     "start_time": "2025-09-07T14:47:05.713295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-08 16:06:36,624 INFO [ds_logger.py:68] [Cached AthenaApi] Local disk-caching is ENABLED.\n",
      "2025-09-08 16:06:36,625 INFO [ds_logger.py:68] [Cached AthenaApi] Cache miss for execute_fetch(args=(\"\\nWITH enrichment_data AS (\\n    SELECT  f.subid,\\n            process_name,\\n            f.partner_name,\\n            f.rn,\\n            transaction_month_prt,\\n            company,\\n            MIN(f.rn) OVER (PARTITION BY subid) AS min_rn,\\n            MAX(f.loanamount)  AS loan_amount, MAX(f.creditscore) AS credit_score \\n\\n    FROM dlk_visitor_funnel_dwh_production.enrich_conversions_flatten f\\n    WHERE f.partner_id in (3158,3178)\\n    AND process_name in ('quicken_rocket_allocated')\\n     AND transaction_month_prt >= '2025-01'\\n     AND f.vertical_id in ('5fa2b415c91a2010c3432900')\\n\\n    GROUP BY 1,2,3,4,5,6\\n)\\nSELECT\\n       rn,\\n       COUNT(DISTINCT subid) AS cids,\\n       COUNT(subid)          AS rowss\\nFROM enrichment_data\\nWHERE rn = min_rn\\nGROUP BY 1\\nORDER BY 2 DESC\\n\",), kwargs={}).\n",
      "fetching manifest from s3://aws-athena-query-results-925511037392-us-east-1/Unsaved/2025/09/08/262bb197-de68-45b7-8c8f-352924ba3183-manifest.csv\n",
      "2025-09-08 16:06:45,400 INFO [ds_logger.py:68] the function _execute_unload was executed in 8.77358 seconds\n",
      "2025-09-08 16:06:45,564 INFO [ds_logger.py:68] the function _execute_fetch was executed in 8.93763 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rn</th>\n",
       "      <th>cids</th>\n",
       "      <th>rowss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>82127</td>\n",
       "      <td>82136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rn   cids  rowss\n",
       "0   1  82127  82136"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_data1(\n",
    "    partner_ids,\n",
    "    process_names,              # e.g. ['quicken_rocket_allocated','quicken_rocket_leads']\n",
    "    transaction_month_prt,      # e.g. '2025-02' (or None to derive from start_date)\n",
    "    vertical_ids,\n",
    "    start_date,                 # e.g. '2025-08-01'\n",
    "    end_date,                   # not used in this query, kept for params symmetry\n",
    "    cond1_col=None,             # e.g. 'loanpurpose'\n",
    "    cond1_val=None              # e.g. 'Refinance' or '%%' to skip\n",
    ") -> pd.DataFrame:\n",
    "    # --- Build WHERE for the CTE ---\n",
    "    query = f\"\"\"\n",
    "WITH enrichment_data AS (\n",
    "    SELECT  f.subid,\n",
    "            process_name,\n",
    "            f.partner_name,\n",
    "            f.rn,\n",
    "            transaction_month_prt,\n",
    "            company,\n",
    "            MIN(f.rn) OVER (PARTITION BY subid) AS min_rn,\n",
    "            {max_enrichment_cols}\n",
    "\n",
    "    FROM dlk_visitor_funnel_dwh_production.enrich_conversions_flatten f\n",
    "    WHERE f.partner_id in ({','.join(map(str, partner_ids))})\n",
    "    AND process_name in ({','.join(f\"'{p}'\" for p in process_names)})\n",
    "     AND transaction_month_prt >= '{transaction_month_prt}'\n",
    "     AND f.vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    "\n",
    "    GROUP BY 1,2,3,4,5,6\n",
    ")\n",
    "SELECT\n",
    "       rn,\n",
    "       COUNT(DISTINCT subid) AS cids,\n",
    "       COUNT(subid)          AS rowss\n",
    "FROM enrichment_data\n",
    "WHERE rn = min_rn\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "\"\"\"\n",
    "    return pd.DataFrame(CachedAthenaApi().execute_fetch(query))\n",
    "\n",
    "df1 = get_data1(\n",
    "    partner_ids=partner_ids,\n",
    "    process_names=process_names,\n",
    "    transaction_month_prt=transaction_month_prt,\n",
    "    vertical_ids=vertical_ids,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date\n",
    ")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475543e0fffcd1fa",
   "metadata": {},
   "source": [
    "### Query 2: checking dates Last Updates (Enrichment Action Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c371dc11c66fec53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T15:27:42.219223Z",
     "start_time": "2025-09-07T15:27:42.216156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-08 16:06:45,585 INFO [ds_logger.py:68] [Cached AthenaApi] Local disk-caching is ENABLED.\n",
      "2025-09-08 16:06:45,586 INFO [ds_logger.py:68] [Cached AthenaApi] Using cached results for execute_fetch(args=(\"\\nSELECT\\n  CAST(SUBSTRING(action_time, 1, 10) AS DATE) AS action_day,\\n  COUNT(DISTINCT subid) AS subids\\nFROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\\nWHERE partner_id in (3158,3178)\\nAND process_name in ('quicken_rocket_allocated')\\n AND transaction_month_prt >= '2025-01'\\n AND vertical_id in ('5fa2b415c91a2010c3432900')\\nGROUP BY 1\\nORDER BY 1 DESC\\n\",), kwargs={}).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_day</th>\n",
       "      <th>subids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-07</td>\n",
       "      <td>1671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>9532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-01</td>\n",
       "      <td>7732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_day  subids\n",
       "0  2025-09-07    1671\n",
       "1  2025-09-01    9532\n",
       "2  2025-08-26       1\n",
       "3  2025-08-14       1\n",
       "4  2025-08-01    7732"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data2(\n",
    "    partner_ids,\n",
    "    process_names,\n",
    "    transaction_month_prt,\n",
    "    vertical_ids,\n",
    "    start_date,\n",
    "    end_date\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    query = f\"\"\"\n",
    "SELECT\n",
    "  CAST(SUBSTRING(action_time, 1, 10) AS DATE) AS action_day,\n",
    "  COUNT(DISTINCT subid) AS subids\n",
    "FROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\n",
    "WHERE partner_id in ({','.join(map(str, partner_ids))})\n",
    "AND process_name in ({','.join(f\"'{p}'\" for p in process_names)})\n",
    " AND transaction_month_prt >= '{transaction_month_prt}'\n",
    " AND vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    "GROUP BY 1\n",
    "ORDER BY 1 DESC\n",
    "\"\"\"\n",
    "    return pd.DataFrame(CachedAthenaApi().execute_fetch(query))\n",
    "\n",
    "df2 = get_data2(\n",
    "    partner_ids=partner_ids,\n",
    "    process_names=process_names,\n",
    "    transaction_month_prt=transaction_month_prt,\n",
    "    vertical_ids=vertical_ids,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date\n",
    ")\n",
    "df2.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed228ff8",
   "metadata": {},
   "source": [
    "Most subids are updated in the last day! The ones that has previous rows data is because the values changed so they have bigger RN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5af35c",
   "metadata": {},
   "source": [
    "### Query 3: Per creation date counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a7d528b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-08 16:06:45,604 INFO [ds_logger.py:68] [Cached AthenaApi] Local disk-caching is ENABLED.\n",
      "2025-09-08 16:06:45,608 INFO [ds_logger.py:68] [Cached AthenaApi] Cache miss for execute_fetch(args=(\"\\nSELECT\\n  CAST(SUBSTRING(created_at, 1, 10) AS DATE) AS created_at_day,\\n  COUNT(DISTINCT subid) AS subids\\nFROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\\nWHERE partner_id in (3158,3178)\\nAND process_name in ('quicken_rocket_allocated')\\n AND transaction_month_prt >= '2025-01'\\n AND vertical_id in ('5fa2b415c91a2010c3432900')\\n -- and rn = 1\\nGROUP BY 1\\nORDER BY 1 DESC\\n\",), kwargs={}).\n",
      "fetching manifest from s3://aws-athena-query-results-925511037392-us-east-1/Unsaved/2025/09/08/dee9be1d-6212-4540-8c80-4a63ccc3b7dc-manifest.csv\n",
      "2025-09-08 16:06:50,493 INFO [ds_logger.py:68] the function _execute_unload was executed in 4.87444 seconds\n",
      "2025-09-08 16:06:50,657 INFO [ds_logger.py:68] the function _execute_fetch was executed in 5.03782 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at_day</th>\n",
       "      <th>subids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-07</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-06</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-04</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_at_day  subids\n",
       "0     2025-09-07     210\n",
       "1     2025-09-06     320\n",
       "2     2025-09-05     313\n",
       "3     2025-09-04     287\n",
       "4     2025-09-03     500"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_data3(\n",
    "    partner_ids,\n",
    "    process_names,\n",
    "    transaction_month_prt,\n",
    "    vertical_ids,\n",
    "    start_date,\n",
    "    end_date\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    query = f\"\"\"\n",
    "SELECT\n",
    "  CAST(SUBSTRING(created_at, 1, 10) AS DATE) AS created_at_day,\n",
    "  COUNT(DISTINCT subid) AS subids\n",
    "FROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\n",
    "WHERE partner_id in ({','.join(map(str, partner_ids))})\n",
    "AND process_name in ({','.join(f\"'{p}'\" for p in process_names)})\n",
    " AND transaction_month_prt >= '{transaction_month_prt}'\n",
    " AND vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    " -- and rn = 1\n",
    "GROUP BY 1\n",
    "ORDER BY 1 DESC\n",
    "\"\"\"\n",
    "    return pd.DataFrame(CachedAthenaApi().execute_fetch(query))\n",
    "\n",
    "df3 = get_data3(\n",
    "    partner_ids=partner_ids,\n",
    "    process_names=process_names,\n",
    "    transaction_month_prt=transaction_month_prt,\n",
    "    vertical_ids=vertical_ids,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date\n",
    ")\n",
    "df3.head()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813a7120",
   "metadata": {},
   "source": [
    "Looks like every day added around 60 +- subids.  looks valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff691cfe084ed9",
   "metadata": {},
   "source": [
    "### Query 4: Hours diff for each field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3b02ec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-08 16:06:50,671 INFO [ds_logger.py:68] [Cached AthenaApi] Local disk-caching is ENABLED.\n",
      "2025-09-08 16:06:50,672 INFO [ds_logger.py:68] [Cached AthenaApi] Using cached results for execute_fetch(args=(\"\\nWITH raw_enrich AS (\\n  SELECT DISTINCT\\n      subid,\\n      process_name,\\n      CAST(SUBSTRING(transaction_date, 1, 10) AS DATE) AS transaction_day,\\n      transaction_date,\\n      transaction_month_prt,\\n      created_at,\\n      loanamount, creditscore, \\n      rn\\n  FROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\\n  WHERE partner_id in (3158,3178)\\n    AND process_name in ('quicken_rocket_allocated')\\n    AND transaction_month_prt between '2025-08' and '2025-09'\\n    AND vertical_id in ('5fa2b415c91a2010c3432900')\\n    AND loanpurpose = 'Refinance'\\n    AND rn = 1\\n),\\nff AS (\\n  SELECT\\n      cid,\\n      conversion_date,\\n      conversion_timestamp,\\n      clickout_timestamp,\\n      company,\\n      SUM(leads_count) AS leads,\\n      SUM(qualified_leads_count) AS qls,\\n      SUM(sales_count) AS sales\\n  FROM dlk_visitor_funnel_dwh_production.chart_funnel\\n  WHERE partner_id in (3158,3178)\\n    AND dt between '2025-08' and '2025-09'\\n    AND vertical_id in ('5fa2b415c91a2010c3432900')\\n    AND traffic_type = 'users'\\n  GROUP BY 1,2,3,4,5\\n  HAVING SUM(leads_count) >= 1\\n),\\ncombined AS (\\n  SELECT\\n      r.subid,\\n      ff.company,\\n      r.transaction_date,\\n      date_format(date_trunc('millisecond', MAX(ff.clickout_timestamp)),\\n        '%Y-%m-%d %H:%i:%s.%f') AS clickout_ts_mt,\\n\\n      date_format( date_trunc( 'millisecond', MIN(CASE WHEN r.creditscore IS NOT NULL THEN\\n              COALESCE(\\n                date_parse(SUBSTRING(r.created_at, 1, 19), '%Y-%m-%dT%H:%i:%s'),\\n                from_iso8601_timestamp(regexp_replace(r.created_at, '([+-]\\\\d2)(\\\\d2)$', '\\\\1:\\\\2'))\\n              )\\n            END\\n          )\\n        ),\\n        '%Y-%m-%d %H:%i:%s.%f'\\n      ) AS creditscore_time,\\n\\n      /* loanamount_time → only when present; parse flexibly, ms-trunc, stringify */\\n      date_format(\\n        date_trunc(\\n          'millisecond',\\n          MIN(\\n            CASE WHEN r.loanamount IS NOT NULL THEN\\n              COALESCE(\\n                date_parse(SUBSTRING(r.created_at, 1, 19), '%Y-%m-%dT%H:%i:%s'),\\n                date_parse(SUBSTRING(r.created_at, 1, 19), '%Y-%m-%d %H:%i:%s'),\\n                from_iso8601_timestamp(regexp_replace(r.created_at, '([+-]\\\\d2)(\\\\d2)$', '\\\\1:\\\\2'))\\n              )\\n            END\\n          )\\n        ),\\n        '%Y-%m-%d %H:%i:%s.%f'\\n      ) AS loanamount_time,\\n      sum(ff.leads) as leads,\\n      sum(ff.qls) as qls,\\n      sum(ff.sales) as sales\\n\\n  FROM raw_enrich r\\n  INNER JOIN ff\\n    ON r.subid = ff.cid\\n   AND r.transaction_day >= ff.conversion_date\\n  GROUP BY r.subid, r.transaction_date, ff.company\\n)\\nSELECT * FROM combined\\n\",), kwargs={}).\n",
      "Added hour difference columns:\n",
      "Columns: ['subid', 'company', 'transaction_date', 'clickout_ts_mt', 'creditscore_time', 'loanamount_time', 'leads', 'qls', 'sales', 'creditscore_diff_hours', 'loanamount_diff_hours']\n",
      "\n",
      "First 5 rows with hour differences:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subid</th>\n",
       "      <th>company</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>clickout_ts_mt</th>\n",
       "      <th>creditscore_time</th>\n",
       "      <th>loanamount_time</th>\n",
       "      <th>leads</th>\n",
       "      <th>qls</th>\n",
       "      <th>sales</th>\n",
       "      <th>creditscore_diff_hours</th>\n",
       "      <th>loanamount_diff_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xiISNkaje1</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-08-30 21:00:00+00:00</td>\n",
       "      <td>2025-08-31 16:26:37+00:00</td>\n",
       "      <td>2025-09-01 18:02:55+00:00</td>\n",
       "      <td>2025-09-01 18:02:55+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaK5O5DG1c</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-08-20 21:00:00+00:00</td>\n",
       "      <td>2025-08-21 22:11:50+00:00</td>\n",
       "      <td>2025-08-22 18:05:42+00:00</td>\n",
       "      <td>2025-08-22 18:05:42+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xmeu7xg3TB</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-08-15 21:00:00+00:00</td>\n",
       "      <td>2025-08-16 19:53:32+00:00</td>\n",
       "      <td>2025-08-17 18:02:07+00:00</td>\n",
       "      <td>2025-08-17 18:02:07+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lms4pwxLh7</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-09-03 21:00:00+00:00</td>\n",
       "      <td>2025-08-30 09:07:26+00:00</td>\n",
       "      <td>2025-09-05 18:00:51+00:00</td>\n",
       "      <td>2025-09-05 18:00:51+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kfas0v0f2f</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-08-19 21:00:00+00:00</td>\n",
       "      <td>2025-08-20 12:43:07+00:00</td>\n",
       "      <td>2025-08-21 18:01:32+00:00</td>\n",
       "      <td>2025-08-21 18:01:32+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subid company          transaction_date            clickout_ts_mt  \\\n",
       "0  xiISNkaje1      ni 2025-08-30 21:00:00+00:00 2025-08-31 16:26:37+00:00   \n",
       "1  NaK5O5DG1c      ni 2025-08-20 21:00:00+00:00 2025-08-21 22:11:50+00:00   \n",
       "2  Xmeu7xg3TB      ni 2025-08-15 21:00:00+00:00 2025-08-16 19:53:32+00:00   \n",
       "3  lms4pwxLh7      ni 2025-09-03 21:00:00+00:00 2025-08-30 09:07:26+00:00   \n",
       "4  Kfas0v0f2f      ni 2025-08-19 21:00:00+00:00 2025-08-20 12:43:07+00:00   \n",
       "\n",
       "           creditscore_time           loanamount_time  leads  qls  sales  \\\n",
       "0 2025-09-01 18:02:55+00:00 2025-09-01 18:02:55+00:00      1    0      0   \n",
       "1 2025-08-22 18:05:42+00:00 2025-08-22 18:05:42+00:00      1    0      0   \n",
       "2 2025-08-17 18:02:07+00:00 2025-08-17 18:02:07+00:00      1    0      0   \n",
       "3 2025-09-05 18:00:51+00:00 2025-09-05 18:00:51+00:00      1    0      0   \n",
       "4 2025-08-21 18:01:32+00:00 2025-08-21 18:01:32+00:00      1    0      0   \n",
       "\n",
       "   creditscore_diff_hours  loanamount_diff_hours  \n",
       "0                    25.0                   25.0  \n",
       "1                    19.0                   19.0  \n",
       "2                    22.0                   22.0  \n",
       "3                   152.0                  152.0  \n",
       "4                    29.0                   29.0  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_data3_combined(\n",
    "    partner_ids,\n",
    "    process_names,\n",
    "    transaction_month_prt,  # not used (we derive months from dates; kept for symmetry)\n",
    "    vertical_ids,\n",
    "    start_date,  # 'YYYY-MM-DD'\n",
    "    end_date     # 'YYYY-MM-DD'\n",
    ") -> pd.DataFrame:\n",
    "    tm_start = start_date[:7]\n",
    "    tm_end   = end_date[:7]\n",
    "\n",
    "    query = f\"\"\"\n",
    "WITH raw_enrich AS (\n",
    "  SELECT DISTINCT\n",
    "      subid,\n",
    "      process_name,\n",
    "      CAST(SUBSTRING(transaction_date, 1, 10) AS DATE) AS transaction_day,\n",
    "      transaction_date,\n",
    "      transaction_month_prt,\n",
    "      created_at,\n",
    "      {enrichment_cols}\n",
    "      rn\n",
    "  FROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\n",
    "  WHERE partner_id in ({','.join(map(str, partner_ids))})\n",
    "    AND process_name in ({','.join(f\"'{p}'\" for p in process_names)})\n",
    "    AND transaction_month_prt between '{tm_start}' and '{tm_end}'\n",
    "    AND vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    "    AND loanpurpose = 'Refinance'\n",
    "    AND rn = 1\n",
    "),\n",
    "ff AS (\n",
    "  SELECT\n",
    "      cid,\n",
    "      conversion_date,\n",
    "      conversion_timestamp,\n",
    "      clickout_timestamp,\n",
    "      company,\n",
    "      SUM(leads_count) AS leads,\n",
    "      SUM(qualified_leads_count) AS qls,\n",
    "      SUM(sales_count) AS sales\n",
    "  FROM dlk_visitor_funnel_dwh_production.chart_funnel\n",
    "  WHERE partner_id in ({','.join(map(str, partner_ids))})\n",
    "    AND dt between '{tm_start}' and '{tm_end}'\n",
    "    AND vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    "    AND traffic_type = 'users'\n",
    "  GROUP BY 1,2,3,4,5\n",
    "  HAVING SUM(leads_count) >= 1\n",
    "),\n",
    "combined AS (\n",
    "  SELECT\n",
    "      r.subid,\n",
    "      ff.company,\n",
    "      r.transaction_date,\n",
    "      date_format(date_trunc('millisecond', MAX(ff.clickout_timestamp)),\n",
    "        '%Y-%m-%d %H:%i:%s.%f') AS clickout_ts_mt,\n",
    "\n",
    "      date_format( date_trunc( 'millisecond', MIN(CASE WHEN r.creditscore IS NOT NULL THEN\n",
    "              COALESCE(\n",
    "                date_parse(SUBSTRING(r.created_at, 1, 19), '%Y-%m-%dT%H:%i:%s'),\n",
    "                from_iso8601_timestamp(regexp_replace(r.created_at, '([+-]\\\\d{2})(\\\\d{2})$', '\\\\1:\\\\2'))\n",
    "              )\n",
    "            END\n",
    "          )\n",
    "        ),\n",
    "        '%Y-%m-%d %H:%i:%s.%f'\n",
    "      ) AS creditscore_time,\n",
    "\n",
    "      /* loanamount_time → only when present; parse flexibly, ms-trunc, stringify */\n",
    "      date_format(\n",
    "        date_trunc(\n",
    "          'millisecond',\n",
    "          MIN(\n",
    "            CASE WHEN r.loanamount IS NOT NULL THEN\n",
    "              COALESCE(\n",
    "                date_parse(SUBSTRING(r.created_at, 1, 19), '%Y-%m-%dT%H:%i:%s'),\n",
    "                date_parse(SUBSTRING(r.created_at, 1, 19), '%Y-%m-%d %H:%i:%s'),\n",
    "                from_iso8601_timestamp(regexp_replace(r.created_at, '([+-]\\\\d{2})(\\\\d{2})$', '\\\\1:\\\\2'))\n",
    "              )\n",
    "            END\n",
    "          )\n",
    "        ),\n",
    "        '%Y-%m-%d %H:%i:%s.%f'\n",
    "      ) AS loanamount_time,\n",
    "      sum(ff.leads) as leads,\n",
    "      sum(ff.qls) as qls,\n",
    "      sum(ff.sales) as sales\n",
    "      \n",
    "  FROM raw_enrich r\n",
    "  INNER JOIN ff\n",
    "    ON r.subid = ff.cid\n",
    "   AND r.transaction_day >= ff.conversion_date\n",
    "  GROUP BY r.subid, r.transaction_date, ff.company\n",
    ")\n",
    "SELECT * FROM combined\n",
    "\"\"\"\n",
    "    return pd.DataFrame(CachedAthenaApi().execute_fetch(query))\n",
    "\n",
    "combined_df = get_data3_combined(\n",
    "    partner_ids=partner_ids,\n",
    "    process_names=process_names,\n",
    "    transaction_month_prt='2025-08',   # ignored for month range; kept for signature symmetry\n",
    "    vertical_ids=vertical_ids,\n",
    "    start_date='2025-08-01',\n",
    "    end_date='2025-09-06'\n",
    ")\n",
    "\n",
    "# parse the string timestamps before hours math (your build_hours_summary already does this)\n",
    "for col in [\"clickout_ts_mt\", \"creditscore_time\", \"loanamount_time\", \"transaction_date\"]:\n",
    "    combined_df[col] = pd.to_datetime(combined_df[col], errors=\"coerce\", utc=True)\n",
    "\n",
    "# Add hour difference columns for all columns ending with '_time'\n",
    "def add_hour_diffs(df, reference_col='clickout_ts_mt'):\n",
    "    \"\"\"\n",
    "    Add hour difference columns for all columns ending with '_time'\n",
    "    compared to the reference column (default: clickout_ts_mt)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Find all columns ending with '_time'\n",
    "    time_cols = [col for col in df.columns if col.endswith('_time')]\n",
    "    \n",
    "    for time_col in time_cols:\n",
    "        # Create diff column name\n",
    "        diff_col = time_col.replace('_time', '_diff_hours')\n",
    "        \n",
    "        # Calculate hour difference: (time_col - reference_col) in hours\n",
    "        # Floor the result and ensure non-negative values\n",
    "        time_diff = (df[time_col] - df[reference_col]).dt.total_seconds() / 3600.0\n",
    "        time_diff = np.floor(time_diff)  # Floor like TIMESTAMPDIFF('hour',...)\n",
    "        time_diff = np.maximum(time_diff, 0)  # Ensure non-negative (GREATEST(..., 0))\n",
    "        \n",
    "        # Handle NaN values (when either timestamp is null)\n",
    "        time_diff = np.where(df[time_col].isna() | df[reference_col].isna(), np.nan, time_diff)\n",
    "        \n",
    "        df[diff_col] = time_diff\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to add hour difference columns\n",
    "combined_df = add_hour_diffs(combined_df)\n",
    "\n",
    "print(\"Added hour difference columns:\")\n",
    "print(\"Columns:\", combined_df.columns.tolist())\n",
    "print(\"\\nFirst 5 rows with hour differences:\")\n",
    "combined_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054469d9",
   "metadata": {},
   "source": [
    "### Query 5: aggregated hours diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ecd86324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using hour columns: ['creditscore_diff_hours', 'loanamount_diff_hours']\n",
      "Sales column available: True\n",
      "\n",
      "============================================================\n",
      "NULL RATES PER FIELD:\n",
      "============================================================\n",
      "Field: CREDITSCORE_DIFF_HOURS\n",
      "  Row null rate: 0.0000 (0.00%)\n",
      "  User null rate: 0.0000 (0.00%)\n",
      "  Filled rows: 4663 / 4663\n",
      "  Users with value: 4663 / 4663\n",
      "  Average sales: 0.02\n",
      "  Total sales: 106\n",
      "----------------------------------------\n",
      "Field: LOANAMOUNT_DIFF_HOURS\n",
      "  Row null rate: 0.0000 (0.00%)\n",
      "  User null rate: 0.0000 (0.00%)\n",
      "  Filled rows: 4663 / 4663\n",
      "  Users with value: 4663 / 4663\n",
      "  Average sales: 0.02\n",
      "  Total sales: 106\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "SUMMARY STATISTICS:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>field_name</th>\n",
       "      <th>avg_hours</th>\n",
       "      <th>median_hours</th>\n",
       "      <th>p80_hours</th>\n",
       "      <th>p90_hours</th>\n",
       "      <th>filled_rows</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>null_rate_rows</th>\n",
       "      <th>users_with_value</th>\n",
       "      <th>total_users</th>\n",
       "      <th>null_rate_users</th>\n",
       "      <th>avg_sales</th>\n",
       "      <th>sum_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ni</td>\n",
       "      <td>CREDITSCORE_DIFF_HOURS</td>\n",
       "      <td>31.364787</td>\n",
       "      <td>25.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4663</td>\n",
       "      <td>4663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4663</td>\n",
       "      <td>4663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022732</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ni</td>\n",
       "      <td>LOANAMOUNT_DIFF_HOURS</td>\n",
       "      <td>31.364787</td>\n",
       "      <td>25.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4663</td>\n",
       "      <td>4663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4663</td>\n",
       "      <td>4663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022732</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company              field_name  avg_hours  median_hours  p80_hours  \\\n",
       "0      ni  CREDITSCORE_DIFF_HOURS  31.364787          25.0       31.0   \n",
       "1      ni   LOANAMOUNT_DIFF_HOURS  31.364787          25.0       31.0   \n",
       "\n",
       "   p90_hours  filled_rows  total_rows  null_rate_rows  users_with_value  \\\n",
       "0       46.0         4663        4663             0.0              4663   \n",
       "1       46.0         4663        4663             0.0              4663   \n",
       "\n",
       "   total_users  null_rate_users  avg_sales  sum_sales  \n",
       "0         4663              0.0   0.022732        106  \n",
       "1         4663              0.0   0.022732        106  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed version with sales stats included\n",
    "def compute_field_stats_fixed(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Find all columns ending with '_diff_hours' or '_hours'\n",
    "    hour_cols = [col for col in df.columns if col.endswith('_diff_hours') or col.endswith('_hours')]\n",
    "    \n",
    "    if not hour_cols:\n",
    "        print(\"No hour columns found. Available columns:\", df.columns.tolist())\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Using hour columns: {hour_cols}\")\n",
    "    \n",
    "    # Check if sales column exists\n",
    "    has_sales = 'sales' in df.columns\n",
    "    print(f\"Sales column available: {has_sales}\")\n",
    "    \n",
    "    long_df = df.melt(\n",
    "        id_vars=[\"subid\", \"company\", \"transaction_date\", \"clickout_ts_mt\"] + ([\"sales\"] if has_sales else []),\n",
    "        value_vars=hour_cols,\n",
    "        var_name=\"field_name\",\n",
    "        value_name=\"hours\"\n",
    "    )\n",
    "    long_df[\"field_name\"] = long_df[\"field_name\"].str.upper()\n",
    "\n",
    "    g = long_df.groupby([\"company\", \"field_name\"], dropna=False)\n",
    "    avg_hours     = g[\"hours\"].mean()\n",
    "    median_hours  = g[\"hours\"].quantile(0.50, interpolation=\"linear\")\n",
    "    p80_hours     = g[\"hours\"].quantile(0.80, interpolation=\"linear\")\n",
    "    p90_hours     = g[\"hours\"].quantile(0.90, interpolation=\"linear\")\n",
    "    filled_rows   = g[\"hours\"].count()\n",
    "    total_rows    = g.size()\n",
    "\n",
    "    users_with_value = long_df[~long_df[\"hours\"].isna()].groupby([\"company\",\"field_name\"])[\"subid\"].nunique()\n",
    "    total_users      = long_df.groupby([\"company\",\"field_name\"])[\"subid\"].nunique()\n",
    "\n",
    "    # Calculate sales stats if sales column exists\n",
    "    if has_sales:\n",
    "        avg_sales = g[\"sales\"].mean()\n",
    "        sum_sales = g[\"sales\"].sum()\n",
    "        \n",
    "        out = pd.concat(\n",
    "            [avg_hours.rename(\"avg_hours\"),\n",
    "             median_hours.rename(\"median_hours\"),\n",
    "             p80_hours.rename(\"p80_hours\"),\n",
    "             p90_hours.rename(\"p90_hours\"),\n",
    "             filled_rows.rename(\"filled_rows\"),\n",
    "             total_rows.rename(\"total_rows\"),\n",
    "             users_with_value.rename(\"users_with_value\"),\n",
    "             total_users.rename(\"total_users\"),\n",
    "             avg_sales.rename(\"avg_sales\"),\n",
    "             sum_sales.rename(\"sum_sales\")],\n",
    "            axis=1\n",
    "        ).reset_index()\n",
    "        \n",
    "        out[\"null_rate_rows\"]  = (1 - (out[\"filled_rows\"] / out[\"total_rows\"].replace(0, np.nan))).round(4)\n",
    "        out[\"null_rate_users\"] = (1 - (out[\"users_with_value\"] / out[\"total_users\"].replace(0, np.nan))).round(4)\n",
    "\n",
    "        out = out[[\"company\",\"field_name\",\"avg_hours\",\"median_hours\",\"p80_hours\",\"p90_hours\",\n",
    "                   \"filled_rows\",\"total_rows\",\"null_rate_rows\",\"users_with_value\",\"total_users\",\"null_rate_users\",\n",
    "                   \"avg_sales\",\"sum_sales\"]]\\\n",
    "                .sort_values([\"company\",\"field_name\"]).reset_index(drop=True)\n",
    "    else:\n",
    "        out = pd.concat(\n",
    "            [avg_hours.rename(\"avg_hours\"),\n",
    "             median_hours.rename(\"median_hours\"),\n",
    "             p80_hours.rename(\"p80_hours\"),\n",
    "             p90_hours.rename(\"p90_hours\"),\n",
    "             filled_rows.rename(\"filled_rows\"),\n",
    "             total_rows.rename(\"total_rows\"),\n",
    "             users_with_value.rename(\"users_with_value\"),\n",
    "             total_users.rename(\"total_users\")],\n",
    "            axis=1\n",
    "        ).reset_index()\n",
    "        \n",
    "        out[\"null_rate_rows\"]  = (1 - (out[\"filled_rows\"] / out[\"total_rows\"].replace(0, np.nan))).round(4)\n",
    "        out[\"null_rate_users\"] = (1 - (out[\"users_with_value\"] / out[\"total_users\"].replace(0, np.nan))).round(4)\n",
    "\n",
    "        out = out[[\"company\",\"field_name\",\"avg_hours\",\"median_hours\",\"p80_hours\",\"p90_hours\",\n",
    "                   \"filled_rows\",\"total_rows\",\"null_rate_rows\",\"users_with_value\",\"total_users\",\"null_rate_users\"]]\\\n",
    "                .sort_values([\"company\",\"field_name\"]).reset_index(drop=True)\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Use the fixed function\n",
    "field_stats = compute_field_stats_fixed(combined_df)    \n",
    "\n",
    "# Print null rates per field\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NULL RATES PER FIELD:\")\n",
    "print(\"=\"*60)\n",
    "for _, row in field_stats.iterrows():\n",
    "    print(f\"Field: {row['field_name']}\")\n",
    "    print(f\"  Row null rate: {row['null_rate_rows']:.4f} ({row['null_rate_rows']*100:.2f}%)\")\n",
    "    print(f\"  User null rate: {row['null_rate_users']:.4f} ({row['null_rate_users']*100:.2f}%)\")\n",
    "    print(f\"  Filled rows: {row['filled_rows']} / {row['total_rows']}\")\n",
    "    print(f\"  Users with value: {row['users_with_value']} / {row['total_users']}\")\n",
    "    if 'avg_sales' in row:\n",
    "        print(f\"  Average sales: {row['avg_sales']:.2f}\")\n",
    "        print(f\"  Total sales: {row['sum_sales']:.0f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS:\")\n",
    "print(\"=\"*60)\n",
    "field_stats.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b15726",
   "metadata": {},
   "source": [
    "### Query 6: for general data for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "00cd4eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-08 16:20:22,015 INFO [ds_logger.py:68] [Cached AthenaApi] Local disk-caching is ENABLED.\n",
      "2025-09-08 16:20:22,016 INFO [ds_logger.py:68] [Cached AthenaApi] Cache miss for execute_fetch(args=(\"\\nWITH raw_enrich AS (\\n  SELECT DISTINCT\\n      subid,\\n      process_name,\\n      transaction_date,\\n      CAST(SUBSTRING(transaction_date, 1, 10) AS DATE) AS transaction_day,\\n      transaction_month_prt,\\n      loanamount, creditscore, \\n      row_number () over (partition by subid order by transaction_date desc) as rnn\\n\\n  FROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\\n  WHERE partner_id in (3158,3178)\\n    AND process_name in ('quicken_rocket_allocated')\\n    AND transaction_month_prt between '2025-08' and '2025-09'\\n    AND vertical_id in ('5fa2b415c91a2010c3432900')\\n    AND rn = 1 \\n    AND loanpurpose = 'Refinance'\\n),\\nff AS (\\n  SELECT\\n      cid,\\n      conversion_date,\\n      conversion_timestamp,\\n      company,\\n      SUM(leads_count) AS leads,\\n      SUM(qualified_leads_count) AS qls,\\n      SUM(sales_count) AS sales\\n\\n  FROM dlk_visitor_funnel_dwh_production.chart_funnel\\n  WHERE partner_id in (3158,3178)\\n    AND dt between '2025-08' and '2025-09'\\n    AND vertical_id in ('5fa2b415c91a2010c3432900')\\n    AND traffic_type = 'users'\\n  GROUP BY 1,2,3,4\\n  HAVING SUM(leads_count) >= 1\\n),\\ncombined AS (\\n  SELECT\\n      coalesce(ff.cid, r.subid) as subid,\\n      ff.company,\\n      r.transaction_date,\\n      loanamount, creditscore, \\n      sum(ff.leads) as leads,\\n      sum(ff.qls) as qls,\\n      sum(ff.sales) as sales\\n\\n  FROM ff \\n  LEFT JOIN raw_enrich r\\n    ON r.subid = ff.cid\\n   AND r.transaction_day >= ff.conversion_date and rnn = 1 \\n  GROUP BY 1,2,3,4,5\\n)\\nSELECT * FROM combined \",), kwargs={}).\n",
      "fetching manifest from s3://aws-athena-query-results-925511037392-us-east-1/Unsaved/2025/09/08/aa725471-a429-4557-bedc-58f99adda3f0-manifest.csv\n",
      "2025-09-08 16:20:31,000 INFO [ds_logger.py:68] the function _execute_unload was executed in 8.98211 seconds\n",
      "2025-09-08 16:20:31,849 INFO [ds_logger.py:68] the function _execute_fetch was executed in 9.83111 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             subid company          transaction_date  loanamount  \\\n",
       "0      YhF18RwGVb      ni  2025-08-14T00:00:00+0300    805000.0   \n",
       "1      buMP1gUMPf      ni  2025-08-18T00:00:00+0300    355000.0   \n",
       "2      j3RwW11LsG      ni  2025-08-18T00:00:00+0300    118000.0   \n",
       "3      Mc9wywjc17      ni  2025-08-29T00:00:00+0300    159000.0   \n",
       "4      suDz2ypGg6      ni  2025-08-11T00:00:00+0300     60000.0   \n",
       "...           ...     ...                       ...         ...   \n",
       "22003  xk1ew3gbFc      ni  2025-08-06T00:00:00+0300    202800.0   \n",
       "22004  fCoKu0gpok      ni  2025-08-01T00:00:00+0300     50000.0   \n",
       "22005  UTjjyzCsw5      ni  2025-08-27T00:00:00+0300    127000.0   \n",
       "22006  uu7ZYgR7wq      ni  2025-08-12T00:00:00+0300    311000.0   \n",
       "22007  lshw0JvTcA      ni  2025-08-17T00:00:00+0300     60000.0   \n",
       "\n",
       "         creditscore  leads  qls  sales  \n",
       "0               Good      1    0      0  \n",
       "1          Excellent      1    0      0  \n",
       "2               Good      1    0      0  \n",
       "3          Excellent      1    0      0  \n",
       "4               Good      1    0      0  \n",
       "...              ...    ...  ...    ...  \n",
       "22003           Good      1    0      0  \n",
       "22004        Average      1    0      0  \n",
       "22005  Below Average      1    0      0  \n",
       "22006      Excellent      1    0      0  \n",
       "22007      Excellent      1    0      0  \n",
       "\n",
       "[22008 rows x 8 columns]>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_general_data(\n",
    "    partner_ids,\n",
    "    process_names,\n",
    "    transaction_month_prt,  # not used (we derive months from dates; kept for symmetry)\n",
    "    vertical_ids,\n",
    "    start_date,  # 'YYYY-MM-DD'\n",
    "    end_date     # 'YYYY-MM-DD'\n",
    ") -> pd.DataFrame:\n",
    "    tm_start = start_date[:7]\n",
    "    tm_end   = end_date[:7]\n",
    "\n",
    "    query = f\"\"\"\n",
    "WITH raw_enrich AS (\n",
    "  SELECT DISTINCT\n",
    "      subid,\n",
    "      process_name,\n",
    "      transaction_date,\n",
    "      CAST(SUBSTRING(transaction_date, 1, 10) AS DATE) AS transaction_day,\n",
    "      transaction_month_prt,\n",
    "      {enrichment_cols}\n",
    "      row_number () over (partition by subid order by transaction_date desc) as rnn\n",
    "\n",
    "  FROM DLK_VISITOR_FUNNEL_DWH_PRODUCTION.ENRICH_CONVERSIONS_FLATTEN\n",
    "  WHERE partner_id in ({','.join(map(str, partner_ids))})\n",
    "    AND process_name in ({','.join(f\"'{p}'\" for p in process_names)})\n",
    "    AND transaction_month_prt between '{tm_start}' and '{tm_end}'\n",
    "    AND vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    "    AND rn = 1 \n",
    "    AND loanpurpose = 'Refinance'\n",
    "),\n",
    "ff AS (\n",
    "  SELECT\n",
    "      cid,\n",
    "      conversion_date,\n",
    "      conversion_timestamp,\n",
    "      company,\n",
    "      SUM(leads_count) AS leads,\n",
    "      SUM(qualified_leads_count) AS qls,\n",
    "      SUM(sales_count) AS sales\n",
    "\n",
    "  FROM dlk_visitor_funnel_dwh_production.chart_funnel\n",
    "  WHERE partner_id in ({','.join(map(str, partner_ids))})\n",
    "    AND dt between '{tm_start}' and '{tm_end}'\n",
    "    AND vertical_id in ({','.join(f\"'{v}'\" for v in vertical_ids)})\n",
    "    AND traffic_type = 'users'\n",
    "  GROUP BY 1,2,3,4\n",
    "  HAVING SUM(leads_count) >= 1\n",
    "),\n",
    "combined AS (\n",
    "  SELECT\n",
    "      coalesce(ff.cid, r.subid) as subid,\n",
    "      ff.company,\n",
    "      r.transaction_date,\n",
    "      {enrichment_cols}\n",
    "      sum(ff.leads) as leads,\n",
    "      sum(ff.qls) as qls,\n",
    "      sum(ff.sales) as sales\n",
    "      \n",
    "  FROM ff \n",
    "  LEFT JOIN raw_enrich r\n",
    "    ON r.subid = ff.cid\n",
    "   AND r.transaction_day >= ff.conversion_date and rnn = 1 \n",
    "  GROUP BY 1,2,3,4,5\n",
    ")\n",
    "SELECT * FROM combined \"\"\"\n",
    "\n",
    "    return pd.DataFrame(CachedAthenaApi().execute_fetch(query))\n",
    "\n",
    "analysis_df = get_general_data(\n",
    "    partner_ids=partner_ids,\n",
    "    process_names=process_names,\n",
    "    transaction_month_prt='2025-08',   # ignored for month range; kept for signature symmetry\n",
    "    vertical_ids=vertical_ids,\n",
    "    start_date='2025-08-01',\n",
    "    end_date='2025-09-06'\n",
    ")\n",
    "\n",
    "analysis_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d3676441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subid</th>\n",
       "      <th>company</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>loanamount</th>\n",
       "      <th>creditscore</th>\n",
       "      <th>leads</th>\n",
       "      <th>qls</th>\n",
       "      <th>sales</th>\n",
       "      <th>l2s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17406</th>\n",
       "      <td>H6tVMjzhF7</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-09-03T00:00:00+0300</td>\n",
       "      <td>240000.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16081</th>\n",
       "      <td>5dBAiZv3XM</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YhF18RwGVb</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-08-14T00:00:00+0300</td>\n",
       "      <td>805000.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14668</th>\n",
       "      <td>OEeJQh0xjc</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-08-04T00:00:00+0300</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14676</th>\n",
       "      <td>0euLnXu3dG</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14675</th>\n",
       "      <td>IUYgxjjq2n</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14674</th>\n",
       "      <td>hC2cFTJc7Y</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14673</th>\n",
       "      <td>QhRa1eP3Sg</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-08-04T00:00:00+0300</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>Average</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14672</th>\n",
       "      <td>dKYiSGM8vZ</td>\n",
       "      <td>ni</td>\n",
       "      <td>2025-08-14T00:00:00+0300</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>Average</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14671</th>\n",
       "      <td>a7EL6jDrpd</td>\n",
       "      <td>ni</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            subid company          transaction_date  loanamount creditscore  \\\n",
       "17406  H6tVMjzhF7      ni  2025-09-03T00:00:00+0300    240000.0        Good   \n",
       "16081  5dBAiZv3XM      ni                      None         NaN        None   \n",
       "0      YhF18RwGVb      ni  2025-08-14T00:00:00+0300    805000.0        Good   \n",
       "14668  OEeJQh0xjc      ni  2025-08-04T00:00:00+0300    800000.0        Good   \n",
       "14676  0euLnXu3dG      ni                      None         NaN        None   \n",
       "14675  IUYgxjjq2n      ni                      None         NaN        None   \n",
       "14674  hC2cFTJc7Y      ni                      None         NaN        None   \n",
       "14673  QhRa1eP3Sg      ni  2025-08-04T00:00:00+0300    450000.0     Average   \n",
       "14672  dKYiSGM8vZ      ni  2025-08-14T00:00:00+0300     10000.0     Average   \n",
       "14671  a7EL6jDrpd      ni                      None         NaN        None   \n",
       "\n",
       "       leads  qls  sales  l2s  \n",
       "17406      2    0      0  0.0  \n",
       "16081      2    0      0  0.0  \n",
       "0          1    0      0  0.0  \n",
       "14668      1    0      0  0.0  \n",
       "14676      1    0      0  0.0  \n",
       "14675      1    0      0  0.0  \n",
       "14674      1    0      0  0.0  \n",
       "14673      1    0      0  0.0  \n",
       "14672      1    0      0  0.0  \n",
       "14671      1    0      0  0.0  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df.sort_values(by='leads', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e666f46",
   "metadata": {},
   "source": [
    "#### Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c1005564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. NULL RATES FOR ENRICHMENT FIELDS\n",
      "============================================================\n",
      "Enrichment fields found: ['loanamount', 'creditscore', 'l2s']\n",
      "loanamount:\n",
      "  Total rows: 22008\n",
      "  Null rows: 17345\n",
      "  Filled rows: 4663\n",
      "  Null rate: 78.81%\n",
      "----------------------------------------\n",
      "creditscore:\n",
      "  Total rows: 22008\n",
      "  Null rows: 17345\n",
      "  Filled rows: 4663\n",
      "  Null rate: 78.81%\n",
      "----------------------------------------\n",
      "l2s:\n",
      "  Total rows: 22008\n",
      "  Null rows: 0\n",
      "  Filled rows: 22008\n",
      "  Null rate: 0.00%\n",
      "----------------------------------------\n",
      "\n",
      "SUMMARY TABLE:\n",
      "         field  null_rate_pct  filled_rows  total_rows\n",
      "0   loanamount          78.81         4663       22008\n",
      "1  creditscore          78.81         4663       22008\n",
      "2          l2s           0.00        22008       22008\n"
     ]
    }
   ],
   "source": [
    "# 1. NULL RATES FOR ENRICHMENT FIELDS\n",
    "print(\"=\"*60)\n",
    "print(\"1. NULL RATES FOR ENRICHMENT FIELDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identify enrichment fields (exclude subid, company, transaction_date, leads, qls, sales)\n",
    "enrichment_fields = [col for col in analysis_df.columns if col not in ['subid', 'company', 'transaction_date', 'leads', 'qls', 'sales']]\n",
    "print(f\"Enrichment fields found: {enrichment_fields}\")\n",
    "\n",
    "null_rates = []\n",
    "for field in enrichment_fields:\n",
    "    total_rows = len(analysis_df)\n",
    "    null_rows = analysis_df[field].isnull().sum()\n",
    "    null_rate = (null_rows / total_rows) * 100\n",
    "    \n",
    "    null_rates.append({\n",
    "        'field': field,\n",
    "        'total_rows': total_rows,\n",
    "        'null_rows': null_rows,\n",
    "        'null_rate_pct': null_rate,\n",
    "        'filled_rows': total_rows - null_rows\n",
    "    })\n",
    "    \n",
    "    print(f\"{field}:\")\n",
    "    print(f\"  Total rows: {total_rows}\")\n",
    "    print(f\"  Null rows: {null_rows}\")\n",
    "    print(f\"  Filled rows: {total_rows - null_rows}\")\n",
    "    print(f\"  Null rate: {null_rate:.2f}%\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Create summary DataFrame\n",
    "null_summary = pd.DataFrame(null_rates)\n",
    "print(\"\\nSUMMARY TABLE:\")\n",
    "print(null_summary[['field', 'null_rate_pct', 'filled_rows', 'total_rows']].round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "360985e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "2. SALES AND QLS ANALYSIS BY ENRICHMENT FIELD VALUES\n",
      "============================================================\n",
      "\n",
      "LOANAMOUNT:\n",
      "----------------------------------------\n",
      "Top 10 values by total sales:\n",
      " loanamount  records  total_leads  leads_mean  total_qls  avg_qls_per_record  total_sales  avg_sales_per_record  avg_l2s_rate\n",
      "    50000.0      112          112         1.0          0                 0.0            7                0.0625        0.0625\n",
      "   100000.0      113          113         1.0          0                 0.0            3                0.0265        0.0265\n",
      "   375000.0       18           18         1.0          0                 0.0            3                0.1667        0.1667\n",
      "   293000.0        5            5         1.0          0                 0.0            3                0.6000        0.6000\n",
      "   160000.0       21           21         1.0          0                 0.0            3                0.1429        0.1429\n",
      "   300000.0       63           63         1.0          0                 0.0            2                0.0317        0.0317\n",
      "    35000.0       14           14         1.0          0                 0.0            2                0.1429        0.1429\n",
      "   390000.0       12           12         1.0          0                 0.0            2                0.1667        0.1667\n",
      "    70000.0       23           23         1.0          0                 0.0            2                0.0870        0.0870\n",
      "   155000.0        9            9         1.0          0                 0.0            2                0.2222        0.2222\n",
      "\n",
      "Overall summary for loanamount:\n",
      "  Total records: 22008\n",
      "  Unique values: 1229\n",
      "  Total leads: 22010\n",
      "  Total QLS: 329\n",
      "  Total sales: 107\n",
      "  Overall L2S rate: 0.49%\n",
      "============================================================\n",
      "\n",
      "CREDITSCORE:\n",
      "----------------------------------------\n",
      "Top 10 values by total sales:\n",
      "  creditscore  records  total_leads  leads_mean  total_qls  avg_qls_per_record  total_sales  avg_sales_per_record  avg_l2s_rate\n",
      "    Excellent     2350         2350      1.0000          0                 0.0           64                0.0272        0.0272\n",
      "         Good     1097         1098      1.0009          0                 0.0           33                0.0301        0.0301\n",
      "      Average      641          641      1.0000          0                 0.0            8                0.0125        0.0125\n",
      "Below Average      507          507      1.0000          0                 0.0            1                0.0020        0.0020\n",
      "         Poor       63           63      1.0000          0                 0.0            0                0.0000        0.0000\n",
      "      Unknown        5            5      1.0000          0                 0.0            0                0.0000        0.0000\n",
      "\n",
      "Overall summary for creditscore:\n",
      "  Total records: 22008\n",
      "  Unique values: 6\n",
      "  Total leads: 22010\n",
      "  Total QLS: 329\n",
      "  Total sales: 107\n",
      "  Overall L2S rate: 0.49%\n",
      "============================================================\n",
      "\n",
      "L2S:\n",
      "----------------------------------------\n",
      "Top 10 values by total sales:\n",
      " l2s  records  total_leads  leads_mean  total_qls  avg_qls_per_record  total_sales  avg_sales_per_record  avg_l2s_rate\n",
      " 1.0      107          107      1.0000          1              0.0093          107                   1.0           1.0\n",
      " 0.0    21901        21903      1.0001        328              0.0150            0                   0.0           0.0\n",
      "\n",
      "Overall summary for l2s:\n",
      "  Total records: 22008\n",
      "  Unique values: 2\n",
      "  Total leads: 22010\n",
      "  Total QLS: 329\n",
      "  Total sales: 107\n",
      "  Overall L2S rate: 0.49%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 2. SALES AND QLS MEAN AND SUMS PER ENRICHMENT FIELD VALUES\n",
    "print(\"=\"*60)\n",
    "print(\"2. SALES AND QLS ANALYSIS BY ENRICHMENT FIELD VALUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate L2S (Leads to Sales) conversion rate\n",
    "analysis_df['l2s'] = analysis_df['sales'] / analysis_df['leads'].replace(0, np.nan)\n",
    "\n",
    "for field in enrichment_fields:\n",
    "    print(f\"\\n{field.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Group by field value and calculate metrics\n",
    "    field_stats = analysis_df.groupby(field).agg({\n",
    "        'leads': ['count', 'sum', 'mean'],\n",
    "        'qls': ['sum', 'mean'],\n",
    "        'sales': ['sum', 'mean'],\n",
    "        'l2s': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    field_stats.columns = ['_'.join(col).strip() for col in field_stats.columns]\n",
    "    field_stats = field_stats.reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    field_stats = field_stats.rename(columns={\n",
    "        'leads_count': 'records',\n",
    "        'leads_sum': 'total_leads',\n",
    "        'qls_sum': 'total_qls',\n",
    "        'qls_mean': 'avg_qls_per_record',\n",
    "        'sales_sum': 'total_sales',\n",
    "        'sales_mean': 'avg_sales_per_record',\n",
    "        'l2s_mean': 'avg_l2s_rate'\n",
    "    })\n",
    "    \n",
    "    # Sort by total sales descending\n",
    "    field_stats = field_stats.sort_values('total_sales', ascending=False)\n",
    "    \n",
    "    print(f\"Top 10 values by total sales:\")\n",
    "    print(field_stats.head(10).to_string(index=False))\n",
    "    \n",
    "    # Overall summary for this field\n",
    "    print(f\"\\nOverall summary for {field}:\")\n",
    "    print(f\"  Total records: {len(analysis_df)}\")\n",
    "    print(f\"  Unique values: {analysis_df[field].nunique()}\")\n",
    "    print(f\"  Total leads: {analysis_df['leads'].sum()}\")\n",
    "    print(f\"  Total QLS: {analysis_df['qls'].sum()}\")\n",
    "    print(f\"  Total sales: {analysis_df['sales'].sum()}\")\n",
    "    print(f\"  Overall L2S rate: {(analysis_df['sales'].sum() / analysis_df['leads'].sum() * 100):.2f}%\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6ddb40fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "3. PER COMPANY L2S ANALYSIS\n",
      "============================================================\n",
      "Company Performance Summary:\n",
      "         total_records  total_leads  total_qls  total_sales  avg_l2s_rate  l2s_overall  qls_rate  sales_rate\n",
      "company                                                                                                     \n",
      "ni               22008        22010        329          107        0.0049         0.49      1.49        0.49\n",
      "\n",
      "Overall Summary:\n",
      "  Total companies: 1\n",
      "  Total records: 22008\n",
      "  Total leads: 22010\n",
      "  Total QLS: 329\n",
      "  Total sales: 107\n",
      "  Overall L2S rate: 0.49%\n"
     ]
    }
   ],
   "source": [
    "# 3. PER COMPANY L2S ANALYSIS\n",
    "print(\"=\"*60)\n",
    "print(\"3. PER COMPANY L2S ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Company-level analysis\n",
    "company_stats = analysis_df.groupby('company').agg({\n",
    "    'subid': 'count',\n",
    "    'leads': 'sum',\n",
    "    'qls': 'sum', \n",
    "    'sales': 'sum',\n",
    "    'l2s': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "company_stats = company_stats.rename(columns={\n",
    "    'subid': 'total_records',\n",
    "    'leads': 'total_leads',\n",
    "    'qls': 'total_qls',\n",
    "    'sales': 'total_sales',\n",
    "    'l2s': 'avg_l2s_rate'\n",
    "})\n",
    "\n",
    "# Calculate additional metrics\n",
    "company_stats['l2s_overall'] = (company_stats['total_sales'] / company_stats['total_leads'] * 100).round(2)\n",
    "company_stats['qls_rate'] = (company_stats['total_qls'] / company_stats['total_leads'] * 100).round(2)\n",
    "company_stats['sales_rate'] = (company_stats['total_sales'] / company_stats['total_leads'] * 100).round(2)\n",
    "\n",
    "# Sort by total sales\n",
    "company_stats = company_stats.sort_values('total_sales', ascending=False)\n",
    "\n",
    "print(\"Company Performance Summary:\")\n",
    "print(company_stats.to_string())\n",
    "\n",
    "print(f\"\\nOverall Summary:\")\n",
    "print(f\"  Total companies: {len(company_stats)}\")\n",
    "print(f\"  Total records: {company_stats['total_records'].sum()}\")\n",
    "print(f\"  Total leads: {company_stats['total_leads'].sum()}\")\n",
    "print(f\"  Total QLS: {company_stats['total_qls'].sum()}\")\n",
    "print(f\"  Total sales: {company_stats['total_sales'].sum()}\")\n",
    "print(f\"  Overall L2S rate: {(company_stats['total_sales'].sum() / company_stats['total_leads'].sum() * 100):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "833e73da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4. CORRELATION ANALYSIS AND CATEGORICAL ANALYSIS\n",
      "============================================================\n",
      "\n",
      "LOANAMOUNT:\n",
      "----------------------------------------\n",
      "Field type: Numeric\n",
      "\n",
      "CORRELATION WITH SALES:\n",
      "  loanamount vs sales: -0.0211\n",
      "  loanamount vs leads: -0.0024\n",
      "  loanamount vs qls: nan\n",
      "  loanamount vs l2s: -0.0211\n",
      "============================================================\n",
      "\n",
      "CREDITSCORE:\n",
      "----------------------------------------\n",
      "Field type: Categorical\n",
      "\n",
      "TOP 10 CATEGORIES BY SALES:\n",
      "Category             Leads      QLS        Sales      L2S Rate  \n",
      "----------------------------------------------------------------------\n",
      "Excellent            2350       0          64         2.72      %\n",
      "Good                 1098       0          33         3.01      %\n",
      "Average              641        0          8          1.25      %\n",
      "Below Average        507        0          1          0.20      %\n",
      "Poor                 63         0          0          0.00      %\n",
      "Unknown              5          0          0          0.00      %\n",
      "\n",
      "Total categories: 6\n",
      "Categories with sales > 0: 4\n",
      "============================================================\n",
      "\n",
      "L2S:\n",
      "----------------------------------------\n",
      "Field type: Numeric\n",
      "\n",
      "CORRELATION WITH SALES:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/checking-new-partners-enrichment/lib/python3.11/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/anaconda3/envs/checking-new-partners-enrichment/lib/python3.11/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/var/folders/19/tgcx3l5d53s090pxf4c256w80000gn/T/ipykernel_51093/4245198939.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     26\u001b[39m         numeric_data = analysis_df[[field, \u001b[33m'sales'\u001b[39m, \u001b[33m'leads'\u001b[39m, \u001b[33m'qls'\u001b[39m, \u001b[33m'l2s'\u001b[39m]].dropna()\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(numeric_data) > \u001b[32m1\u001b[39m:\n\u001b[32m     29\u001b[39m             correlations = {\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m                 \u001b[33m'sales'\u001b[39m: numeric_data[field].corr(numeric_data[\u001b[33m'sales'\u001b[39m]),\n\u001b[32m     31\u001b[39m                 \u001b[33m'leads'\u001b[39m: numeric_data[field].corr(numeric_data[\u001b[33m'leads'\u001b[39m]),\n\u001b[32m     32\u001b[39m                 \u001b[33m'qls'\u001b[39m: numeric_data[field].corr(numeric_data[\u001b[33m'qls'\u001b[39m]),\n\u001b[32m     33\u001b[39m                 \u001b[33m'l2s'\u001b[39m: numeric_data[field].corr(numeric_data[\u001b[33m'l2s'\u001b[39m])\n",
      "\u001b[32m/opt/anaconda3/envs/checking-new-partners-enrichment/lib/python3.11/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, method, min_periods, numeric_only)\u001b[39m\n\u001b[32m  11034\u001b[39m         cols = data.columns\n\u001b[32m  11035\u001b[39m         idx = cols.copy()\n\u001b[32m  11036\u001b[39m         mat = data.to_numpy(dtype=float, na_value=np.nan, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m  11037\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m11038\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"pearson\"\u001b[39m:\n\u001b[32m  11039\u001b[39m             correl = libalgos.nancorr(mat, minp=min_periods)\n\u001b[32m  11040\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"spearman\"\u001b[39m:\n\u001b[32m  11041\u001b[39m             correl = libalgos.nancorr_spearman(mat, minp=min_periods)\n",
      "\u001b[32m/opt/anaconda3/envs/checking-new-partners-enrichment/lib/python3.11/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1574\u001b[39m     @final\n\u001b[32m   1575\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m __nonzero__(self) -> NoReturn:\n\u001b[32m-> \u001b[39m\u001b[32m1576\u001b[39m         raise ValueError(\n\u001b[32m   1577\u001b[39m             \u001b[33mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[39m\n\u001b[32m   1578\u001b[39m             \u001b[33m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[39m\n\u001b[32m   1579\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# 4. CORRELATION ANALYSIS AND CATEGORICAL ANALYSIS\n",
    "print(\"=\"*60)\n",
    "print(\"4. CORRELATION ANALYSIS AND CATEGORICAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Function to detect if field is numeric\n",
    "def is_numeric_field(series):\n",
    "    try:\n",
    "        pd.to_numeric(series, errors='raise')\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "for field in enrichment_fields:\n",
    "    print(f\"\\n{field.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check if field is numeric\n",
    "    is_numeric = is_numeric_field(analysis_df[field])\n",
    "    print(f\"Field type: {'Numeric' if is_numeric else 'Categorical'}\")\n",
    "    \n",
    "    if is_numeric:\n",
    "        # Correlation analysis for numeric fields\n",
    "        print(\"\\nCORRELATION WITH SALES:\")\n",
    "        # Remove null values for correlation\n",
    "        numeric_data = analysis_df[[field, 'sales', 'leads', 'qls', 'l2s']].dropna()\n",
    "        \n",
    "        if len(numeric_data) > 1:\n",
    "            correlations = {\n",
    "                'sales': numeric_data[field].corr(numeric_data['sales']),\n",
    "                'leads': numeric_data[field].corr(numeric_data['leads']),\n",
    "                'qls': numeric_data[field].corr(numeric_data['qls']),\n",
    "                'l2s': numeric_data[field].corr(numeric_data['l2s'])\n",
    "            }\n",
    "            \n",
    "            for metric, corr in correlations.items():\n",
    "                print(f\"  {field} vs {metric}: {corr:.4f}\")\n",
    "        else:\n",
    "            print(\"  Not enough data for correlation analysis\")\n",
    "            \n",
    "    else:\n",
    "        # Categorical analysis\n",
    "        print(\"\\nTOP 10 CATEGORIES BY SALES:\")\n",
    "        cat_analysis = analysis_df.groupby(field).agg({\n",
    "            'leads': 'sum',\n",
    "            'qls': 'sum',\n",
    "            'sales': 'sum',\n",
    "            'l2s': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Calculate L2S rate\n",
    "        cat_analysis['l2s_rate'] = (cat_analysis['sales'] / cat_analysis['leads'] * 100).round(2)\n",
    "        \n",
    "        # Sort by sales\n",
    "        cat_analysis = cat_analysis.sort_values('sales', ascending=False)\n",
    "        \n",
    "        # Show top 10\n",
    "        top_10 = cat_analysis.head(10)\n",
    "        print(f\"{'Category':<20} {'Leads':<10} {'QLS':<10} {'Sales':<10} {'L2S Rate':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "        for category, row in top_10.iterrows():\n",
    "            print(f\"{str(category)[:18]:<20} {row['leads']:<10.0f} {row['qls']:<10.0f} {row['sales']:<10.0f} {row['l2s_rate']:<10.2f}%\")\n",
    "        \n",
    "        print(f\"\\nTotal categories: {len(cat_analysis)}\")\n",
    "        print(f\"Categories with sales > 0: {len(cat_analysis[cat_analysis['sales'] > 0])}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bf448677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4. CORRELATION ANALYSIS AND CATEGORICAL ANALYSIS\n",
      "============================================================\n",
      "\n",
      "LOANAMOUNT:\n",
      "----------------------------------------\n",
      "Field type: Numeric\n",
      "\n",
      "CORRELATION WITH SALES:\n",
      "  loanamount vs sales: -0.0211\n",
      "  loanamount vs leads: -0.0024\n",
      "  loanamount vs qls: No correlation (constant values or insufficient data)\n",
      "  loanamount vs l2s: -0.0211\n",
      "============================================================\n",
      "\n",
      "CREDITSCORE:\n",
      "----------------------------------------\n",
      "Field type: Categorical\n",
      "\n",
      "TOP 10 CATEGORIES BY SALES:\n",
      "Category             Leads      QLS        Sales      L2S Rate  \n",
      "----------------------------------------------------------------------\n",
      "Excellent            2350       0          64         2.72      %\n",
      "Good                 1098       0          33         3.01      %\n",
      "Average              641        0          8          1.25      %\n",
      "Below Average        507        0          1          0.20      %\n",
      "Poor                 63         0          0          0.00      %\n",
      "Unknown              5          0          0          0.00      %\n",
      "\n",
      "Total categories: 6\n",
      "Categories with sales > 0: 4\n",
      "\n",
      "Category Performance Insights:\n",
      "  Best L2S rate: 3.01%\n",
      "  Worst L2S rate: 0.00%\n",
      "  Average L2S rate: 1.20%\n",
      "============================================================\n",
      "\n",
      "L2S:\n",
      "----------------------------------------\n",
      "Field type: Numeric\n",
      "\n",
      "CORRELATION WITH SALES:\n",
      "  l2s vs sales: No correlation (constant values or insufficient data)\n",
      "  l2s vs leads: No correlation (constant values or insufficient data)\n",
      "  l2s vs qls: No correlation (constant values or insufficient data)\n",
      "  l2s vs l2s: No correlation (constant values or insufficient data)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 4. CORRELATION ANALYSIS AND CATEGORICAL ANALYSIS (FIXED VERSION)\n",
    "print(\"=\"*60)\n",
    "print(\"4. CORRELATION ANALYSIS AND CATEGORICAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Function to detect if field is numeric\n",
    "def is_numeric_field(series):\n",
    "    try:\n",
    "        pd.to_numeric(series, errors='raise')\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Function to safely calculate correlation\n",
    "def safe_correlation(series1, series2):\n",
    "    try:\n",
    "        # Remove null values\n",
    "        data = pd.DataFrame({'x': series1, 'y': series2}).dropna()\n",
    "        if len(data) < 2:\n",
    "            return np.nan\n",
    "        # Check if either series has zero variance\n",
    "        if data['x'].std() == 0 or data['y'].std() == 0:\n",
    "            return np.nan\n",
    "        return data['x'].corr(data['y'])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "for field in enrichment_fields:\n",
    "    print(f\"\\n{field.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check if field is numeric\n",
    "    is_numeric = is_numeric_field(analysis_df[field])\n",
    "    print(f\"Field type: {'Numeric' if is_numeric else 'Categorical'}\")\n",
    "    \n",
    "    if is_numeric:\n",
    "        # Correlation analysis for numeric fields\n",
    "        print(\"\\nCORRELATION WITH SALES:\")\n",
    "        # Remove null values for correlation\n",
    "        numeric_data = analysis_df[[field, 'sales', 'leads', 'qls', 'l2s']].dropna()\n",
    "        \n",
    "        if len(numeric_data) > 1:\n",
    "            correlations = {\n",
    "                'sales': safe_correlation(numeric_data[field], numeric_data['sales']),\n",
    "                'leads': safe_correlation(numeric_data[field], numeric_data['leads']),\n",
    "                'qls': safe_correlation(numeric_data[field], numeric_data['qls']),\n",
    "                'l2s': safe_correlation(numeric_data[field], numeric_data['l2s'])\n",
    "            }\n",
    "            \n",
    "            for metric, corr in correlations.items():\n",
    "                if np.isnan(corr):\n",
    "                    print(f\"  {field} vs {metric}: No correlation (constant values or insufficient data)\")\n",
    "                else:\n",
    "                    print(f\"  {field} vs {metric}: {corr:.4f}\")\n",
    "        else:\n",
    "            print(\"  Not enough data for correlation analysis\")\n",
    "            \n",
    "    else:\n",
    "        # Categorical analysis\n",
    "        print(\"\\nTOP 10 CATEGORIES BY SALES:\")\n",
    "        cat_analysis = analysis_df.groupby(field).agg({\n",
    "            'leads': 'sum',\n",
    "            'qls': 'sum',\n",
    "            'sales': 'sum',\n",
    "            'l2s': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Calculate L2S rate safely (avoid division by zero)\n",
    "        cat_analysis['l2s_rate'] = np.where(\n",
    "            cat_analysis['leads'] > 0,\n",
    "            (cat_analysis['sales'] / cat_analysis['leads'] * 100).round(2),\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Sort by sales\n",
    "        cat_analysis = cat_analysis.sort_values('sales', ascending=False)\n",
    "        \n",
    "        # Show top 10\n",
    "        top_10 = cat_analysis.head(10)\n",
    "        print(f\"{'Category':<20} {'Leads':<10} {'QLS':<10} {'Sales':<10} {'L2S Rate':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "        for category, row in top_10.iterrows():\n",
    "            print(f\"{str(category)[:18]:<20} {row['leads']:<10.0f} {row['qls']:<10.0f} {row['sales']:<10.0f} {row['l2s_rate']:<10.2f}%\")\n",
    "        \n",
    "        print(f\"\\nTotal categories: {len(cat_analysis)}\")\n",
    "        print(f\"Categories with sales > 0: {len(cat_analysis[cat_analysis['sales'] > 0])}\")\n",
    "        \n",
    "        # Additional insights for categorical data\n",
    "        print(f\"\\nCategory Performance Insights:\")\n",
    "        print(f\"  Best L2S rate: {cat_analysis[cat_analysis['leads'] > 0]['l2s_rate'].max():.2f}%\")\n",
    "        print(f\"  Worst L2S rate: {cat_analysis[cat_analysis['leads'] > 0]['l2s_rate'].min():.2f}%\")\n",
    "        print(f\"  Average L2S rate: {cat_analysis[cat_analysis['leads'] > 0]['l2s_rate'].mean():.2f}%\")\n",
    "    \n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bba33046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING FILTERED DATASET FOR CORRELATION ANALYSIS\n",
      "============================================================\n",
      "Original dataset size: 22008\n",
      "Filtered dataset size (all enrichment fields non-null): 4663\n",
      "Records removed: 17345\n",
      "Percentage of data retained: 21.19%\n",
      "\n",
      "Null rates in filtered dataset:\n",
      "  loanamount: 0 nulls (0.00%)\n",
      "  creditscore: 0 nulls (0.00%)\n",
      "\n",
      "Filtered dataset summary:\n",
      "  Total leads: 4664\n",
      "  Total QLS: 0\n",
      "  Total sales: 106\n",
      "  Overall L2S rate: 2.27%\n"
     ]
    }
   ],
   "source": [
    "# Create filtered dataset with only non-null enrichment values\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING FILTERED DATASET FOR CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create analysis_df_inner with only records where ALL enrichment fields are not null\n",
    "analysis_df_inner = analysis_df.dropna(subset=enrichment_fields)\n",
    "\n",
    "print(f\"Original dataset size: {len(analysis_df)}\")\n",
    "print(f\"Filtered dataset size (all enrichment fields non-null): {len(analysis_df_inner)}\")\n",
    "print(f\"Records removed: {len(analysis_df) - len(analysis_df_inner)}\")\n",
    "print(f\"Percentage of data retained: {len(analysis_df_inner) / len(analysis_df) * 100:.2f}%\")\n",
    "\n",
    "# Show null rates in the filtered dataset\n",
    "print(f\"\\nNull rates in filtered dataset:\")\n",
    "for field in enrichment_fields:\n",
    "    null_count = analysis_df_inner[field].isnull().sum()\n",
    "    print(f\"  {field}: {null_count} nulls ({null_count / len(analysis_df_inner) * 100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nFiltered dataset summary:\")\n",
    "print(f\"  Total leads: {analysis_df_inner['leads'].sum()}\")\n",
    "print(f\"  Total QLS: {analysis_df_inner['qls'].sum()}\")\n",
    "print(f\"  Total sales: {analysis_df_inner['sales'].sum()}\")\n",
    "print(f\"  Overall L2S rate: {(analysis_df_inner['sales'].sum() / analysis_df_inner['leads'].sum() * 100):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0a2706ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CORRELATION ANALYSIS WITH FILTERED DATASET\n",
      "============================================================\n",
      "\n",
      "LOANAMOUNT:\n",
      "----------------------------------------\n",
      "Field type: Numeric\n",
      "Records with non-null loanamount: 4663\n",
      "\n",
      "CORRELATION WITH SALES (filtered dataset):\n",
      "  loanamount vs sales: -0.0211\n",
      "  loanamount vs leads: -0.0024\n",
      "  loanamount vs qls: No correlation (constant values or insufficient data)\n",
      "  loanamount vs l2s: -0.0211\n",
      "\n",
      "Numeric Field Insights:\n",
      "  Min value: 0.0\n",
      "  Max value: 13233459.0\n",
      "  Mean value: 337542.04\n",
      "  Median value: 247500.00\n",
      "  Std deviation: 583334.01\n",
      "============================================================\n",
      "\n",
      "CREDITSCORE:\n",
      "----------------------------------------\n",
      "Field type: Categorical\n",
      "Records with non-null creditscore: 4663\n",
      "\n",
      "TOP 10 CATEGORIES BY SALES (filtered dataset):\n",
      "Category             Leads      QLS        Sales      L2S Rate  \n",
      "----------------------------------------------------------------------\n",
      "Excellent            2350       0          64         2.72      %\n",
      "Good                 1098       0          33         3.01      %\n",
      "Average              641        0          8          1.25      %\n",
      "Below Average        507        0          1          0.20      %\n",
      "Poor                 63         0          0          0.00      %\n",
      "Unknown              5          0          0          0.00      %\n",
      "\n",
      "Total categories: 6\n",
      "Categories with sales > 0: 4\n",
      "\n",
      "Category Performance Insights:\n",
      "  Best L2S rate: 3.01%\n",
      "  Worst L2S rate: 0.00%\n",
      "  Average L2S rate: 1.20%\n",
      "============================================================\n",
      "\n",
      "L2S:\n",
      "----------------------------------------\n",
      "Field type: Numeric\n",
      "Records with non-null l2s: 4663\n",
      "\n",
      "CORRELATION WITH SALES (filtered dataset):\n",
      "  l2s vs sales: 1.0000\n",
      "  l2s vs leads: -0.0022\n",
      "  l2s vs qls: No correlation (constant values or insufficient data)\n",
      "  l2s vs l2s: 1.0000\n",
      "\n",
      "Numeric Field Insights:\n",
      "  Min value: 0.0\n",
      "  Max value: 1.0\n",
      "  Mean value: 0.02\n",
      "  Median value: 0.00\n",
      "  Std deviation: 0.15\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CORRELATION ANALYSIS WITH FILTERED DATASET\n",
    "print(\"=\"*60)\n",
    "print(\"CORRELATION ANALYSIS WITH FILTERED DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Function to safely calculate correlation\n",
    "def safe_correlation(series1, series2):\n",
    "    try:\n",
    "        # Remove null values\n",
    "        data = pd.DataFrame({'x': series1, 'y': series2}).dropna()\n",
    "        if len(data) < 2:\n",
    "            return np.nan\n",
    "        # Check if either series has zero variance\n",
    "        if data['x'].std() == 0 or data['y'].std() == 0:\n",
    "            return np.nan\n",
    "        return data['x'].corr(data['y'])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Function to detect if field is numeric\n",
    "def is_numeric_field(series):\n",
    "    try:\n",
    "        pd.to_numeric(series, errors='raise')\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "for field in enrichment_fields:\n",
    "    print(f\"\\n{field.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check if field is numeric\n",
    "    is_numeric = is_numeric_field(analysis_df_inner[field])\n",
    "    print(f\"Field type: {'Numeric' if is_numeric else 'Categorical'}\")\n",
    "    print(f\"Records with non-null {field}: {len(analysis_df_inner)}\")\n",
    "    \n",
    "    if is_numeric:\n",
    "        # Correlation analysis for numeric fields using filtered data\n",
    "        print(\"\\nCORRELATION WITH SALES (filtered dataset):\")\n",
    "        \n",
    "        correlations = {\n",
    "            'sales': safe_correlation(analysis_df_inner[field], analysis_df_inner['sales']),\n",
    "            'leads': safe_correlation(analysis_df_inner[field], analysis_df_inner['leads']),\n",
    "            'qls': safe_correlation(analysis_df_inner[field], analysis_df_inner['qls']),\n",
    "            'l2s': safe_correlation(analysis_df_inner[field], analysis_df_inner['l2s'])\n",
    "        }\n",
    "        \n",
    "        for metric, corr in correlations.items():\n",
    "            if np.isnan(corr):\n",
    "                print(f\"  {field} vs {metric}: No correlation (constant values or insufficient data)\")\n",
    "            else:\n",
    "                print(f\"  {field} vs {metric}: {corr:.4f}\")\n",
    "                \n",
    "        # Additional insights for numeric fields\n",
    "        print(f\"\\nNumeric Field Insights:\")\n",
    "        print(f\"  Min value: {analysis_df_inner[field].min()}\")\n",
    "        print(f\"  Max value: {analysis_df_inner[field].max()}\")\n",
    "        print(f\"  Mean value: {analysis_df_inner[field].mean():.2f}\")\n",
    "        print(f\"  Median value: {analysis_df_inner[field].median():.2f}\")\n",
    "        print(f\"  Std deviation: {analysis_df_inner[field].std():.2f}\")\n",
    "        \n",
    "    else:\n",
    "        # Categorical analysis using filtered data\n",
    "        print(\"\\nTOP 10 CATEGORIES BY SALES (filtered dataset):\")\n",
    "        cat_analysis = analysis_df_inner.groupby(field).agg({\n",
    "            'leads': 'sum',\n",
    "            'qls': 'sum',\n",
    "            'sales': 'sum',\n",
    "            'l2s': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Calculate L2S rate safely (avoid division by zero)\n",
    "        cat_analysis['l2s_rate'] = np.where(\n",
    "            cat_analysis['leads'] > 0,\n",
    "            (cat_analysis['sales'] / cat_analysis['leads'] * 100).round(2),\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Sort by sales\n",
    "        cat_analysis = cat_analysis.sort_values('sales', ascending=False)\n",
    "        \n",
    "        # Show top 10\n",
    "        top_10 = cat_analysis.head(10)\n",
    "        print(f\"{'Category':<20} {'Leads':<10} {'QLS':<10} {'Sales':<10} {'L2S Rate':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "        for category, row in top_10.iterrows():\n",
    "            print(f\"{str(category)[:18]:<20} {row['leads']:<10.0f} {row['qls']:<10.0f} {row['sales']:<10.0f} {row['l2s_rate']:<10.2f}%\")\n",
    "        \n",
    "        print(f\"\\nTotal categories: {len(cat_analysis)}\")\n",
    "        print(f\"Categories with sales > 0: {len(cat_analysis[cat_analysis['sales'] > 0])}\")\n",
    "        \n",
    "        # Additional insights for categorical data\n",
    "        print(f\"\\nCategory Performance Insights:\")\n",
    "        print(f\"  Best L2S rate: {cat_analysis[cat_analysis['leads'] > 0]['l2s_rate'].max():.2f}%\")\n",
    "        print(f\"  Worst L2S rate: {cat_analysis[cat_analysis['leads'] > 0]['l2s_rate'].min():.2f}%\")\n",
    "        print(f\"  Average L2S rate: {cat_analysis[cat_analysis['leads'] > 0]['l2s_rate'].mean():.2f}%\")\n",
    "    \n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8ce670b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INVESTIGATION: Leads vs Records Discrepancy\n",
      "============================================================\n",
      "Total records: 22008\n",
      "Total leads: 22010\n",
      "Average leads per record: 1.0001\n",
      "\n",
      "Leads distribution:\n",
      "leads\n",
      "1    22006\n",
      "2        2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Records with leads > 1:\n",
      "Count: 2\n",
      "Percentage: 0.01%\n",
      "\n",
      "Top records with highest leads:\n",
      "            subid  leads  qls  sales\n",
      "16081  5dBAiZv3XM      2    0      0\n",
      "17406  H6tVMjzhF7      2    0      0\n",
      "\n",
      "Leads statistics:\n",
      "Min: 1\n",
      "Max: 2\n",
      "Median: 1.0\n",
      "Std: 0.01\n",
      "\n",
      "Records with 0 leads: 0\n"
     ]
    }
   ],
   "source": [
    "# INVESTIGATION: Leads vs Records Discrepancy\n",
    "print(\"=\"*60)\n",
    "print(\"INVESTIGATION: Leads vs Records Discrepancy\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Total records: {len(analysis_df)}\")\n",
    "print(f\"Total leads: {analysis_df['leads'].sum()}\")\n",
    "print(f\"Average leads per record: {analysis_df['leads'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nLeads distribution:\")\n",
    "print(analysis_df['leads'].value_counts().sort_index().head(20))\n",
    "\n",
    "print(f\"\\nRecords with leads > 1:\")\n",
    "leads_gt_1 = analysis_df[analysis_df['leads'] > 1]\n",
    "print(f\"Count: {len(leads_gt_1)}\")\n",
    "print(f\"Percentage: {len(leads_gt_1) / len(analysis_df) * 100:.2f}%\")\n",
    "\n",
    "if len(leads_gt_1) > 0:\n",
    "    print(f\"\\nTop records with highest leads:\")\n",
    "    print(leads_gt_1[['subid', 'leads', 'qls', 'sales']].sort_values('leads', ascending=False).head(10))\n",
    "\n",
    "print(f\"\\nLeads statistics:\")\n",
    "print(f\"Min: {analysis_df['leads'].min()}\")\n",
    "print(f\"Max: {analysis_df['leads'].max()}\")\n",
    "print(f\"Median: {analysis_df['leads'].median()}\")\n",
    "print(f\"Std: {analysis_df['leads'].std():.2f}\")\n",
    "\n",
    "# Check if there are any records with 0 leads\n",
    "zero_leads = analysis_df[analysis_df['leads'] == 0]\n",
    "print(f\"\\nRecords with 0 leads: {len(zero_leads)}\")\n",
    "if len(zero_leads) > 0:\n",
    "    print(\"This might explain the discrepancy - some records have 0 leads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2238b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. NULL RATES FOR ENRICHMENT FIELDS\n",
    "print(\"=\"*60)\n",
    "print(\"1. NULL RATES FOR ENRICHMENT FIELDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identify enrichment fields (exclude subid, company, transaction_date, leads, qls, sales, l2s)\n",
    "enrichment_fields = [col for col in analysis_df.columns if col not in ['subid', 'company', 'transaction_date', 'leads', 'qls', 'sales', 'l2s']]\n",
    "print(f\"Enrichment fields found: {enrichment_fields}\")\n",
    "\n",
    "# Fill NaN values in l2s with 0\n",
    "analysis_df['l2s'] = analysis_df['l2s'].fillna(0)\n",
    "\n",
    "null_rates = []\n",
    "for field in enrichment_fields:\n",
    "    total_rows = len(analysis_df)\n",
    "    null_rows = analysis_df[field].isnull().sum()\n",
    "    null_rate = (null_rows / total_rows) * 100\n",
    "    \n",
    "    null_rates.append({\n",
    "        'field': field,\n",
    "        'total_rows': total_rows,\n",
    "        'null_rows': null_rows,\n",
    "        'null_rate_pct': null_rate,\n",
    "        'filled_rows': total_rows - null_rows\n",
    "    })\n",
    "    \n",
    "    print(f\"{field}:\")\n",
    "    print(f\"  Total rows: {total_rows}\")\n",
    "    print(f\"  Null rows: {null_rows}\")\n",
    "    print(f\"  Filled rows: {total_rows - null_rows}\")\n",
    "    print(f\"  Null rate: {null_rate:.2f}%\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Create summary DataFrame\n",
    "null_summary = pd.DataFrame(null_rates)\n",
    "print(\"\\nSUMMARY TABLE:\")\n",
    "print(null_summary[['field', 'null_rate_pct', 'filled_rows', 'total_rows']].round(2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checking-new-partners-enrichment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
